{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SF001-123456/the404thinkers/blob/main/model_training/DistilBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage 1: Binary Classification"
      ],
      "metadata": {
        "id": "7NpdSblKa7ys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup and Prepare Data"
      ],
      "metadata": {
        "id": "T5iP89UrDf2y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8UFsEBDPAOaT",
        "outputId": "c6d10b45-4ae9-4191-e9a5-d9545a30ae69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.5)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.34.4)\n",
            "Requirement already satisfied: transformers[touch] in /usr/local/lib/python3.12/dist-packages (4.55.4)\n",
            "\u001b[33mWARNING: transformers 4.55.4 does not provide the extra 'touch'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers[touch]) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers[touch]) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers[touch]) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers[touch]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers[touch]) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers[touch]) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers[touch]) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers[touch]) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers[touch]) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.8)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[touch]) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[touch]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[touch]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[touch]) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Install library\n",
        "!pip install transformers[touch] datasets evaluate huggingface_hub\n",
        "# !pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Load data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_PATH = '/content/drive/MyDrive/datathon/comment data/'\n",
        "BALANCE_FILE = os.path.join(DRIVE_PATH, 'balanced(34290).csv')\n",
        "\n",
        "df = pd.read_csv(BALANCE_FILE)\n",
        "print(f\"Loaded {len(df)} rows from merged_comments.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THw2SDklFf4v",
        "outputId": "2e57e73d-0c43-4137-a9fa-f6a02891ab03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Loaded 68580 rows from merged_comments.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Only need cleaned_text and label\n",
        "df = df[['cleaned_text', 'isProductRelated']]\n",
        "df = df.rename(columns={'isProductRelated': 'label', 'cleaned_text': 'text'})\n",
        "\n",
        "# Double check dataset quality\n",
        "df.dropna(subset=['text'], inplace=True)\n",
        "\n",
        "# Split data into training and testing (0.2)\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
        "\n",
        "# Convert pd DF to Hugging Face Dataset objects\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Combine into DatasetDict\n",
        "ds = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'test': test_dataset\n",
        "})\n",
        "\n",
        "print(ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGP3wQ_MGO0D",
        "outputId": "4d774d74-0259-4009-f72c-cc70f4d0dc9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label', '__index_level_0__'],\n",
            "        num_rows: 53760\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label', '__index_level_0__'],\n",
            "        num_rows: 13441\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing and Tokenization"
      ],
      "metadata": {
        "id": "0JURRUiMHOCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the tokenizer for DistilBERT\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Create a function to tokenize text\n",
        "def preprocess_function(examples):\n",
        "  # The tokenizer will pad and truncate the text to a standard length\n",
        "  return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "# Apply tokenization to dataset\n",
        "tokenized_ds = ds.map(preprocess_function, batched=True)\n",
        "\n",
        "print(tokenized_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "aOT2tTkEHR27",
        "outputId": "8c765417-abfb-4f28-f073-adf4429d2f5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/53760 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/13441 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
            "        num_rows: 53760\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
            "        num_rows: 13441\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Model and Training Arguments (without LoRA)"
      ],
      "metadata": {
        "id": "X3zBeJscIkT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "# Define metrics\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "precision = evaluate.load(\"precision\")\n",
        "recall = evaluate.load(\"recall\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "  predictions, labels = eval_pred\n",
        "  predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "  acc = accuracy.compute(predictions=predictions, references=labels)\n",
        "  prec = precision.compute(predictions=predictions, references=labels, average=\"binary\")\n",
        "  rec = recall.compute(predictions=predictions, references=labels, average=\"binary\")\n",
        "  f1_score = f1.compute(predictions=predictions, references=labels, average=\"binary\")\n",
        "\n",
        "  return {\n",
        "      \"accuracy\": acc[\"accuracy\"],\n",
        "      \"precision\": prec[\"precision\"],\n",
        "      \"recall\": rec[\"recall\"],\n",
        "      \"f1\": f1_score[\"f1\"],\n",
        "  }\n",
        "\n",
        "# Define labels\n",
        "id2label = {0: \"Not Product Related\", 1: \"Product Related\"}\n",
        "label2id = {\"Not Product Related\": 0, \"Product Related\": 1}\n",
        "\n",
        "# Load RoBERTa\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id\n",
        ")\n",
        "\n",
        "# Define HF Hub model repo\n",
        "hub_model_id = \"sainoforce/distilbert-base-product-related\"\n",
        "\n",
        "# Define Training Argument\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=hub_model_id,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\", # Evaluate at the end of each epoch\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\",\n",
        "    push_to_hub=True, # This will automatically push the model to the Hub\n",
        ")\n",
        "\n",
        "# Create Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_ds[\"train\"],\n",
        "    eval_dataset=tokenized_ds[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Start Training\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "ggG1auCgImyI",
        "outputId": "d30fa926-ace3-4dc6-cdd9-9c9425b314da"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-2863614936.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10080' max='10080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10080/10080 31:32, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.015500</td>\n",
              "      <td>0.007315</td>\n",
              "      <td>0.998810</td>\n",
              "      <td>0.999270</td>\n",
              "      <td>0.998396</td>\n",
              "      <td>0.998833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.004300</td>\n",
              "      <td>0.006174</td>\n",
              "      <td>0.998586</td>\n",
              "      <td>0.997961</td>\n",
              "      <td>0.999271</td>\n",
              "      <td>0.998616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.007619</td>\n",
              "      <td>0.998958</td>\n",
              "      <td>0.998688</td>\n",
              "      <td>0.999271</td>\n",
              "      <td>0.998980</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10080, training_loss=0.01337463751775171, metrics={'train_runtime': 1894.1859, 'train_samples_per_second': 85.145, 'train_steps_per_second': 5.322, 'total_flos': 5341085513809920.0, 'train_loss': 0.01337463751775171, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Model and Training Arguments (with LoRA)"
      ],
      "metadata": {
        "id": "ki1V_wUVneYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "f7D1C_Efnikk",
        "outputId": "c50946e5-cd88-4361-953c-b9abfeee37ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft) (4.55.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.10.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.6.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.34.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (0.21.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "# Load Tokenizer and Model\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwifqNfhnosu",
        "outputId": "92a00093-d5a6-4a22-a80b-ee634940aea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the DistilBERT model first\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
        ")\n",
        "\n",
        "# Print the model structure to see the layer names\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dH-Gm8lMzpGu",
        "outputId": "e0ca3353-3900-429e-b9da-9025f01e3590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBertForSequenceClassification(\n",
            "  (distilbert): DistilBertModel(\n",
            "    (embeddings): Embeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): Transformer(\n",
            "      (layer): ModuleList(\n",
            "        (0-5): 6 x TransformerBlock(\n",
            "          (attention): DistilBertSdpaAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (activation): GELUActivation()\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup LoRA\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS, # Sequence Classification\n",
        "    r=8,                        # Rank (can experiment with 16)\n",
        "    lora_alpha=16,              # Scaling factor (often 2x rank)\n",
        "    target_modules=[\"q_lin\", \"v_lin\"], # Target the attention layers\n",
        "    lora_dropout=0.1,\n",
        ")\n",
        "\n",
        "# Wrap the model with LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2euCCeHoUuZ",
        "outputId": "d79c28cb-512d-414e-ad13-61b329baae59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 739,586 || all params: 67,694,596 || trainable%: 1.0925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Training Arguments\n",
        "hub_model_id = \"sainoforce/distilbert-base-product-related\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=hub_model_id,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",  # It's good to evaluate to track progress\n",
        "    load_best_model_at_end=True,  # Important for getting the best version\n",
        "    metric_for_best_model=\"f1\",   # Use F1 as the deciding metric\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",\n",
        "    push_to_hub=True, # Push to Hub at the end\n",
        ")\n",
        "\n",
        "# Define Metrics\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "recall_metric = evaluate.load(\"recall\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy_metric.compute(predictions=predictions, references=labels)['accuracy'],\n",
        "        'precision': precision_metric.compute(predictions=predictions, references=labels, average='binary')['precision'],\n",
        "        'recall': recall_metric.compute(predictions=predictions, references=labels, average='binary')['recall'],\n",
        "        'f1': f1_metric.compute(predictions=predictions, references=labels, average='binary')['f1'],\n",
        "    }\n",
        "\n",
        "# Create Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_ds['train'], # Make sure you're using your tokenized datasets\n",
        "    eval_dataset=tokenized_ds['test'],\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer, # Pass the tokenizer to save it with the model\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "10j8cfsyolsB",
        "outputId": "2562f6ad-f3a6-49cd-9c2d-d34c015b3fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2955605291.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9938' max='10080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 9938/10080 21:51 < 00:18, 7.58 it/s, Epoch 2.96/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.141200</td>\n",
              "      <td>0.091430</td>\n",
              "      <td>0.968976</td>\n",
              "      <td>0.958565</td>\n",
              "      <td>0.981627</td>\n",
              "      <td>0.969959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.059600</td>\n",
              "      <td>0.069879</td>\n",
              "      <td>0.979763</td>\n",
              "      <td>0.970832</td>\n",
              "      <td>0.990085</td>\n",
              "      <td>0.980364</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10080' max='10080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10080/10080 23:00, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.141200</td>\n",
              "      <td>0.091430</td>\n",
              "      <td>0.968976</td>\n",
              "      <td>0.958565</td>\n",
              "      <td>0.981627</td>\n",
              "      <td>0.969959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.059600</td>\n",
              "      <td>0.069879</td>\n",
              "      <td>0.979763</td>\n",
              "      <td>0.970832</td>\n",
              "      <td>0.990085</td>\n",
              "      <td>0.980364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.070200</td>\n",
              "      <td>0.066222</td>\n",
              "      <td>0.981251</td>\n",
              "      <td>0.973345</td>\n",
              "      <td>0.990376</td>\n",
              "      <td>0.981787</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10080, training_loss=0.13291056709630147, metrics={'train_runtime': 1380.806, 'train_samples_per_second': 116.801, 'train_steps_per_second': 7.3, 'total_flos': 5432692884111360.0, 'train_loss': 0.13291056709630147, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate and Push to HF"
      ],
      "metadata": {
        "id": "zbDVXi4LX-An"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run evaluation on test set\n",
        "final_metrics = trainer.evaluate()\n",
        "print(\"Final Evaluation metrics:\")\n",
        "print(final_metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "2fAEVfg6WjNm",
        "outputId": "c2ec1a6f-0bde-44d3-ea57-ac49889708c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='841' max='841' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [841/841 00:50]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Evaluation metrics:\n",
            "{'eval_loss': 0.06622204929590225, 'eval_accuracy': 0.9812513949854922, 'eval_precision': 0.9733447979363714, 'eval_recall': 0.9903762029746281, 'eval_f1': 0.981786643538595, 'eval_runtime': 51.2716, 'eval_samples_per_second': 262.153, 'eval_steps_per_second': 16.403, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Push final model, tokenizer, and training config to Hub\n",
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "fYTd4S8u2tRu",
        "outputId": "70eb862c-d1c5-4ced-c5b4-907875878ca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload                         : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...e-product-related/training_args.bin: 100%|##########| 5.71kB / 5.71kB            "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...t-related/adapter_model.safetensors: 100%|##########| 2.96MB / 2.96MB            "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...e-product-related/model.safetensors:   6%|6         | 16.8MB /  268MB            "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/junmeng-sf/distilbert-base-product-related/commit/ad15d9d388bdb49244d20c5a5f52b4031524694d', commit_message='End of training', commit_description='', oid='ad15d9d388bdb49244d20c5a5f52b4031524694d', pr_url=None, repo_url=RepoUrl('https://huggingface.co/junmeng-sf/distilbert-base-product-related', endpoint='https://huggingface.co', repo_type='model', repo_id='junmeng-sf/distilbert-base-product-related'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "aBcSRspdZx_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"text-classification\", model=hub_model_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHZ3HtmJZzJl",
        "outputId": "5d60bf94-f9c4-4e75-c7b5-66a9e678320d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --- Sample comments ---\n",
        "comments = [\n",
        "    \"This foundation blends so smoothly, perfect for my skin tone.\",\n",
        "    \"The lipstick color is beautiful but it fades too quickly.\",\n",
        "    \"I love how this moisturizer keeps my skin hydrated all day.\",\n",
        "    \"Is this serum safe for sensitive skin?\",\n",
        "    \"The packaging is cute but the mascara dries out too fast.\",\n",
        "    \"This sunscreen leaves a white cast, not good for darker skin tones.\",\n",
        "    \"Best shampoo Iâ€™ve tried, makes my hair feel so soft.\",\n",
        "    \"I bought this face wash and it really reduced my acne.\",\n",
        "    \"The fragrance of this body lotion is too strong for me.\",\n",
        "    \"Does this eye cream actually reduce dark circles?\",\n",
        "    # Non-beauty\n",
        "    \"Great video, thanks for the explanation!\",\n",
        "    \"The background music is really calming.\",\n",
        "    \"I laughed so hard at the blooper at the end.\",\n",
        "    \"Can you make a tutorial about studying tips?\",\n",
        "    \"Subscribed, looking forward to your next video!\",\n",
        "    \"I tried this new thing and it was okayâ€¦\" ,\n",
        "    \"The packaging looks nice but I havenâ€™t used it yet.\",\n",
        "    \"Not sure if this really works as advertised.\",\n",
        "    \"My skin felt weird after using itâ€¦\",\n",
        "    \"This color is amazing!\" ,\n",
        "    \"I usually donâ€™t like these things, but this is different.\" ,\n",
        "    \"Feels smooth, but not sure if Iâ€™ll buy again.\",\n",
        "    \"It smells strong, not really my style.\" ,\n",
        "    \"The effect is subtle, but noticeable.\" ,\n",
        "    \"Everyone says this is good, but I donâ€™t see the difference.\"\n",
        "]\n",
        "\n",
        "# --- Bulk test with classifier ---\n",
        "results = [classifier(c) for c in comments]\n",
        "\n",
        "# --- Save into DataFrame ---\n",
        "df_results = pd.DataFrame({\n",
        "    \"comment\": comments,\n",
        "    \"prediction\": results\n",
        "})\n",
        "\n",
        "print(df_results)\n",
        "\n",
        "# df_results.to_csv(\"/content/drive/MyDrive/datathon/results/beauty_classifier_distilbert_results.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dnb69sLFZ6Vm",
        "outputId": "294c8cf6-a658-4d6c-98f0-c2976fba4350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              comment  \\\n",
            "0   This foundation blends so smoothly, perfect fo...   \n",
            "1   The lipstick color is beautiful but it fades t...   \n",
            "2   I love how this moisturizer keeps my skin hydr...   \n",
            "3              Is this serum safe for sensitive skin?   \n",
            "4   The packaging is cute but the mascara dries ou...   \n",
            "5   This sunscreen leaves a white cast, not good f...   \n",
            "6   Best shampoo Iâ€™ve tried, makes my hair feel so...   \n",
            "7   I bought this face wash and it really reduced ...   \n",
            "8   The fragrance of this body lotion is too stron...   \n",
            "9   Does this eye cream actually reduce dark circles?   \n",
            "10           Great video, thanks for the explanation!   \n",
            "11            The background music is really calming.   \n",
            "12       I laughed so hard at the blooper at the end.   \n",
            "13       Can you make a tutorial about studying tips?   \n",
            "14    Subscribed, looking forward to your next video!   \n",
            "15            I tried this new thing and it was okayâ€¦   \n",
            "16  The packaging looks nice but I havenâ€™t used it...   \n",
            "17       Not sure if this really works as advertised.   \n",
            "18                 My skin felt weird after using itâ€¦   \n",
            "19                             This color is amazing!   \n",
            "20  I usually donâ€™t like these things, but this is...   \n",
            "21      Feels smooth, but not sure if Iâ€™ll buy again.   \n",
            "22             It smells strong, not really my style.   \n",
            "23              The effect is subtle, but noticeable.   \n",
            "24  Everyone says this is good, but I donâ€™t see th...   \n",
            "\n",
            "                                           prediction  \n",
            "0   [{'label': 'LABEL_1', 'score': 0.9999213218688...  \n",
            "1   [{'label': 'LABEL_1', 'score': 0.9999393224716...  \n",
            "2    [{'label': 'LABEL_1', 'score': 0.9719278216362}]  \n",
            "3   [{'label': 'LABEL_1', 'score': 0.9991105198860...  \n",
            "4   [{'label': 'LABEL_1', 'score': 0.9994310736656...  \n",
            "5   [{'label': 'LABEL_1', 'score': 0.9995290040969...  \n",
            "6   [{'label': 'LABEL_1', 'score': 0.9990750551223...  \n",
            "7   [{'label': 'LABEL_1', 'score': 0.6336544752120...  \n",
            "8   [{'label': 'LABEL_1', 'score': 0.9970838427543...  \n",
            "9   [{'label': 'LABEL_1', 'score': 0.9975219368934...  \n",
            "10  [{'label': 'LABEL_0', 'score': 0.9997875094413...  \n",
            "11  [{'label': 'LABEL_0', 'score': 0.9997908473014...  \n",
            "12  [{'label': 'LABEL_0', 'score': 0.9995887875556...  \n",
            "13  [{'label': 'LABEL_0', 'score': 0.9944715499877...  \n",
            "14  [{'label': 'LABEL_0', 'score': 0.9995672106742...  \n",
            "15  [{'label': 'LABEL_0', 'score': 0.9997403025627...  \n",
            "16  [{'label': 'LABEL_0', 'score': 0.9989888072013...  \n",
            "17  [{'label': 'LABEL_0', 'score': 0.9996147155761...  \n",
            "18  [{'label': 'LABEL_0', 'score': 0.9995085000991...  \n",
            "19  [{'label': 'LABEL_0', 'score': 0.6079987287521...  \n",
            "20  [{'label': 'LABEL_0', 'score': 0.9981564879417...  \n",
            "21  [{'label': 'LABEL_0', 'score': 0.9992675185203...  \n",
            "22  [{'label': 'LABEL_0', 'score': 0.9980942606925...  \n",
            "23  [{'label': 'LABEL_0', 'score': 0.9953240752220...  \n",
            "24  [{'label': 'LABEL_0', 'score': 0.9987614154815...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage 2: Multi-Class Classification"
      ],
      "metadata": {
        "id": "uoCrgCP6bAsJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Refined Keyword List"
      ],
      "metadata": {
        "id": "rc28KqdjbDQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_keywords = {\n",
        "    \"makeup\": [\n",
        "        \"makeup\", \"cosmetic\", \"foundation\", \"concealer\", \"primer\", \"blush\", \"bronzer\",\n",
        "        \"highlighter\", \"powder\", \"setting spray\", \"lipstick\", \"lip gloss\", \"lip liner\", \"lip stain\",\n",
        "        \"mascara\", \"eyeliner\", \"eyeshadow\", \"brow pencil\", \"fake lashes\", \"beauty blender\"\n",
        "    ],\n",
        "    \"skincare\": [\n",
        "        \"skincare\", \"cleanser\", \"toner\", \"moisturizer\", \"lotion\", \"cream\", \"serum\",\n",
        "        \"essence\", \"sunscreen\", \"sunblock\", \"spf\", \"sheet mask\", \"clay mask\",\n",
        "        \"exfoliator\", \"peel\", \"retinol\", \"hyaluronic acid\", \"niacinamide\", \"vitamin c\",\n",
        "        \"collagen\", \"peptide\", \"anti-aging\", \"acne\", \"blemish\"\n",
        "    ],\n",
        "    \"haircare\": [\n",
        "        \"shampoo\", \"conditioner\", \"haircare\", \"hair mask\", \"hair oil\", \"hairspray\",\n",
        "        \"mousse\", \"gel\", \"dry shampoo\", \"heat protectant\", \"keratin\", \"leave-in\",\n",
        "        \"scalp treatment\", \"hair treatment\"\n",
        "    ],\n",
        "    \"haircolor\": [\n",
        "        \"hair color\", \"hair dye\", \"dyeing\", \"bleach\", \"highlights\", \"roots\", \"box dye\",\n",
        "        \"color-treated\", \"toner\" # Note: toner can be skincare or haircolor, context matters. Keyword matching is imperfect.\n",
        "    ],\n",
        "    \"fragrance\": [\n",
        "        \"fragrance\", \"perfume\", \"cologne\", \"eau de toilette\", \"scent\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "brand_keywords = [\"loreal\", \"l'orÃ©al\", \"maybelline\", \"garnier\", \"nyx\", \"lancÃ´me\", \"kiehl's\", \"cerave\"]  # maybe can use this at final pipeline"
      ],
      "metadata": {
        "id": "CL3vB-SEcFFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation - Data Labelling"
      ],
      "metadata": {
        "id": "h1i41phZcR-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load dataset\n",
        "DRIVE_PATH = '/content/drive/MyDrive/datathon/comment data/'\n",
        "BALANCE_FILE = os.path.join(DRIVE_PATH, 'balanced(34290).csv')\n",
        "\n",
        "# Load the full balanced dataset\n",
        "df_full = pd.read_csv(BALANCE_FILE)\n",
        "\n",
        "# For Stage 2, only use comments that are product-related\n",
        "df_product_related = df_full[df_full['isProductRelated'] == 1].copy()\n",
        "\n",
        "df_stage2 = df_product_related[['cleaned_text']].copy()\n",
        "\n",
        "print(f\"Original number of comments: {len(df_full)}\")\n",
        "print(f\"Number of product-related comments for Stage 2: {len(df_stage2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABDJ5jLvcXr0",
        "outputId": "3f3955b8-bdbd-4b31-b059-7905cb65410f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of comments: 68580\n",
            "Number of product-related comments for Stage 2: 34290\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define labelling function\n",
        "def assign_category_by_counts(text):\n",
        "  if not isinstance(text, str):\n",
        "    return \"unlabeled\"\n",
        "\n",
        "  text_lower = text.lower()\n",
        "\n",
        "  category_scores = {category: 0 for category in category_keywords.keys()}\n",
        "\n",
        "  # Count keyword occurrences for each category\n",
        "  for category, keywords in category_keywords.items():\n",
        "    for keyword in keywords:\n",
        "        if keyword in text_lower:\n",
        "            category_scores[category] += 1\n",
        "\n",
        "  # Find the category with the highest score\n",
        "  max_score = 0\n",
        "  best_category = \"unlabeled\" # Default if no keywords are found\n",
        "  for category, score in category_scores.items():\n",
        "      if score > max_score:\n",
        "          max_score = score\n",
        "          best_category = category\n",
        "\n",
        "  return best_category\n",
        "\n",
        "# Apply the function to create our new 'category' label column\n",
        "df_stage2['category'] = df_stage2['cleaned_text'].apply(assign_category_by_counts)"
      ],
      "metadata": {
        "id": "-jH4OlH8cqvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of new labels\n",
        "print(\"Category distribution:\")\n",
        "print(df_stage2['category'].value_counts())\n",
        "\n",
        "# Filter out comments that failed to label\n",
        "# Can be caused if comment was marked isProductRelated=1 by a generic term\n",
        "# Such as beauty\n",
        "df_stage2_labeled = df_stage2[df_stage2['category'] != 'unlabeled'].copy()\n",
        "\n",
        "print(f\"\\nTotal labeled comments for Stage 2 training: {len(df_stage2_labeled)}\")\n",
        "print(f\"Dropped {len(df_stage2) - len(df_stage2_labeled)} unlabeled comments.\")\n",
        "\n",
        "# Sanity Check a few samples\n",
        "print(\"\\n--- Sanity Check ---\")\n",
        "for category in category_keywords.keys():\n",
        "    print(f\"\\nExample for category: {category}\")\n",
        "    example_comment = df_stage2_labeled[df_stage2_labeled['category'] == category]['cleaned_text'].iloc[0]\n",
        "    print(example_comment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyH0YfQSem9M",
        "outputId": "4b6aaea5-8b3e-441a-d786-150bad3a969c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Category distribution:\n",
            "category\n",
            "makeup       18294\n",
            "unlabeled     7449\n",
            "skincare      4855\n",
            "haircare      1512\n",
            "haircolor     1221\n",
            "fragrance      959\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Total labeled comments for Stage 2 training: 26841\n",
            "Dropped 7449 unlabeled comments.\n",
            "\n",
            "--- Sanity Check ---\n",
            "\n",
            "Example for category: makeup\n",
            "foundation stick shade please\n",
            "\n",
            "Example for category: skincare\n",
            "girlies like boohoo bf wont skincare love\n",
            "\n",
            "Example for category: haircare\n",
            "girl ok deep condition wig brush conditioner use warm water washing curl youtube tutorial make sure look specifically synthetic hair wig\n",
            "\n",
            "Example for category: haircolor\n",
            "girl seen like video different hair color\n",
            "\n",
            "Example for category: fragrance\n",
            "thats fragrance clerk ghetto like\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Data for Model"
      ],
      "metadata": {
        "id": "kVAlh2P7Ceca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Library\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "qnOfoyoRCXdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create integer labels from the 'category' column\n",
        "labels = df_stage2_labeled['category'].unique().tolist()\n",
        "label2id = {label: i for i, label in enumerate(labels)}\n",
        "id2label = {i: label for i, label in enumerate(labels)}\n",
        "df_stage2_labeled['label'] = df_stage2_labeled['category'].map(label2id)\n",
        "\n",
        "print(\"--- Label Mappings ---\")\n",
        "print(id2label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddgMnmjpCqpF",
        "outputId": "78644405-9d37-4d11-d695-a37f464e2f94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Label Mappings ---\n",
            "{0: 'makeup', 1: 'haircare', 2: 'skincare', 3: 'fragrance', 4: 'haircolor'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the final columns needed: 'text' and 'label'\n",
        "df_for_training = df_stage2_labeled[['cleaned_text', 'label']].rename(columns={'cleaned_text': 'text'})\n",
        "\n",
        "# Split the data into training and testing sets (80/20 split)\n",
        "train_df, test_df = train_test_split(\n",
        "    df_for_training, test_size=0.2, random_state=42, stratify=df_for_training['label']\n",
        ")\n",
        "\n",
        "# Convert to Hugging Face Dataset objects\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "category_ds = DatasetDict({'train': train_dataset, 'test': test_dataset})\n",
        "print(\"\\n--- Final Dataset for Stage 2 ---\")\n",
        "print(category_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-hOK-JNCzah",
        "outputId": "c485307f-9075-430e-f2f5-bbf16f83174b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Dataset for Stage 2 ---\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label', '__index_level_0__'],\n",
            "        num_rows: 21472\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label', '__index_level_0__'],\n",
            "        num_rows: 5369\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "c88fTbzmC-9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_category_ds = category_ds.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "8Lqw-36VC77b",
        "outputId": "3871b5e0-a5c3-4264-8d22-112fa88116ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/21472 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5369 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Multi-Class Model with LoRA"
      ],
      "metadata": {
        "id": "weyqw4oXDHY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels = len(labels) # This will be 5\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q_lin\", \"v_lin\"],\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UvfYMHzDKnJ",
        "outputId": "363ab538-e27d-46c7-ad08-603bfa55125e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 741,893 || all params: 67,699,210 || trainable%: 1.0959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Model"
      ],
      "metadata": {
        "id": "-7bGpYVpDZXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_model_id = \"sainoforce/distilbert-base-category-classifier\"\n",
        "\n",
        "# Define training_args\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=category_model_id,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    report_to=\"none\",\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "# Define Metrics\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "recall_metric = evaluate.load(\"recall\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # *** CRITICAL CHANGE: Use average='weighted' for multi-class ***\n",
        "    # This accounts for class imbalance.\n",
        "    return {\n",
        "        'accuracy': accuracy_metric.compute(predictions=predictions, references=labels)['accuracy'],\n",
        "        'precision': precision_metric.compute(predictions=predictions, references=labels, average='weighted')['precision'],\n",
        "        'recall': recall_metric.compute(predictions=predictions, references=labels, average='weighted')['recall'],\n",
        "        'f1': f1_metric.compute(predictions=predictions, references=labels, average='weighted')['f1'],\n",
        "    }\n",
        "\n",
        "# Define Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_category_ds['train'],\n",
        "    eval_dataset=tokenized_category_ds['test'],\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "_cv_ur4FDbUY",
        "outputId": "37d874ce-7d96-415a-e5bc-e699d2b806dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1398028692.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4026' max='4026' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4026/4026 09:12, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.503200</td>\n",
              "      <td>0.233650</td>\n",
              "      <td>0.927733</td>\n",
              "      <td>0.926507</td>\n",
              "      <td>0.927733</td>\n",
              "      <td>0.926703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.195400</td>\n",
              "      <td>0.162305</td>\n",
              "      <td>0.948966</td>\n",
              "      <td>0.948670</td>\n",
              "      <td>0.948966</td>\n",
              "      <td>0.948721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.154000</td>\n",
              "      <td>0.148169</td>\n",
              "      <td>0.951760</td>\n",
              "      <td>0.951412</td>\n",
              "      <td>0.951760</td>\n",
              "      <td>0.951487</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=4026, training_loss=0.329936290107079, metrics={'train_runtime': 553.0092, 'train_samples_per_second': 116.483, 'train_steps_per_second': 7.28, 'total_flos': 2170071669325824.0, 'train_loss': 0.329936290107079, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate and Push to Hub"
      ],
      "metadata": {
        "id": "b9-tfCygESeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run evaluation on test set\n",
        "final_metrics = trainer.evaluate()\n",
        "print(\"Final Evaluation metrics:\")\n",
        "print(final_metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "e4DbC_osEUlo",
        "outputId": "dfae30ee-a85e-4680-ea45-37a0b0fcdc98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='336' max='336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [336/336 00:20]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Evaluation metrics:\n",
            "{'eval_loss': 0.14816902577877045, 'eval_accuracy': 0.9517601043024772, 'eval_precision': 0.9514121408446373, 'eval_recall': 0.9517601043024772, 'eval_f1': 0.9514870982633408, 'eval_runtime': 20.5618, 'eval_samples_per_second': 261.115, 'eval_steps_per_second': 16.341, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "6B1AJvA9EWNz",
        "outputId": "815ce133-8fc6-4820-d13e-8f6f343a7cbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload                         : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...tegory-classifier/training_args.bin: 100%|##########| 5.78kB / 5.78kB            "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...lassifier/adapter_model.safetensors: 100%|##########| 2.97MB / 2.97MB            "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/junmeng-sf/distilbert-base-category-classifier/commit/183b9796bc465a501065c2003fb650273ffef309', commit_message='End of training', commit_description='', oid='183b9796bc465a501065c2003fb650273ffef309', pr_url=None, repo_url=RepoUrl('https://huggingface.co/junmeng-sf/distilbert-base-category-classifier', endpoint='https://huggingface.co', repo_type='model', repo_id='junmeng-sf/distilbert-base-category-classifier'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference (Stage 1 + Stage 2 with Brand Tagging)"
      ],
      "metadata": {
        "id": "39CiYlYSEZCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from peft import PeftModel\n",
        "\n",
        "# --- Define Hub Model IDs ---\n",
        "hub_id_stage1 = \"sainoforce/distilbert-base-product-related\"\n",
        "hub_id_stage2 = \"sainoforce/distilbert-base-category-classifier\"\n",
        "base_model_name = \"distilbert-base-uncased\"\n",
        "\n",
        "# --- Load Stage 1 Model ---\n",
        "print(\"Loading Stage 1: Product Relevance Classifier...\")\n",
        "# 1. Load the correct base model architecture with the right number of labels\n",
        "base_model_stage1 = AutoModelForSequenceClassification.from_pretrained(\n",
        "    base_model_name,\n",
        "    num_labels=2  # For binary classification (isProductRelated or not)\n",
        ")\n",
        "# 2. Load the LoRA adapter and merge it with the base model\n",
        "model_stage1 = PeftModel.from_pretrained(base_model_stage1, hub_id_stage1)\n",
        "# 3. Load the tokenizer associated with the model\n",
        "tokenizer_stage1 = AutoTokenizer.from_pretrained(hub_id_stage1)\n",
        "# 4. Now, create the pipeline with the fully-formed model and tokenizer\n",
        "classifier_stage1 = pipeline(\"text-classification\", model=model_stage1, tokenizer=tokenizer_stage1)\n",
        "\n",
        "# --- Load Stage 2 Model ---\n",
        "print(\"\\nLoading Stage 2: Category Classifier...\")\n",
        "# 1. Load the base model, but this time with the correct number of category labels\n",
        "base_model_stage2 = AutoModelForSequenceClassification.from_pretrained(\n",
        "    base_model_name,\n",
        "    num_labels=5 # For your 5 categories (makeup, skincare, etc.)\n",
        ")\n",
        "# 2. Load the LoRA adapter for the category classifier\n",
        "model_stage2 = PeftModel.from_pretrained(base_model_stage2, hub_id_stage2)\n",
        "# 3. Load its tokenizer\n",
        "tokenizer_stage2 = AutoTokenizer.from_pretrained(hub_id_stage2)\n",
        "# 4. Create the second pipeline\n",
        "classifier_stage2 = pipeline(\"text-classification\", model=model_stage2, tokenizer=tokenizer_stage2)\n",
        "\n",
        "\n",
        "print(\"\\nModels loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPZmkpeVEaHj",
        "outputId": "81da9854-6509-464a-81ca-cfd093b04e53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Stage 1: Product Relevance Classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading Stage 2: Category Classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Models loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Brand Keywords and Extraction Function\n",
        "brand_keywords = [\n",
        "    \"loreal\", \"l'orÃ©al\", \"maybelline\", \"garnier\", \"nyx\", \"lancÃ´me\",\n",
        "    \"kiehl's\", \"cerave\", \"infallible\", \"elvive\" # You can add product line names too!\n",
        "]\n",
        "\n",
        "def extract_brands(text, brands):\n",
        "    mentioned_brands = []\n",
        "    text_lower = text.lower()\n",
        "    for brand in brands:\n",
        "        if brand in text_lower:\n",
        "            mentioned_brands.append(brand)\n",
        "    # If the list is empty, return a default value\n",
        "    return mentioned_brands if mentioned_brands else [\"Brand Not Specified\"]"
      ],
      "metadata": {
        "id": "TWxMiiQWKaOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stage2_id2label = {\n",
        "    0: 'makeup',\n",
        "    1: 'skincare',\n",
        "    2: 'haircare',\n",
        "    3: 'haircolor',\n",
        "    4: 'fragrance'\n",
        "}\n",
        "\n",
        "# Create End-to-End Pipeline Function\n",
        "def analyze_comment(comment_text):\n",
        "    # Stage 1: Is the comment product-related?\n",
        "    stage1_result = classifier_stage1(comment_text)[0]\n",
        "\n",
        "    # Access the model's config to get the human-readable label\n",
        "    label_id_stage1 = int(stage1_result['label'].split('_')[1])\n",
        "    is_related_label = classifier_stage1.model.config.id2label[label_id_stage1]\n",
        "\n",
        "    # The gatekeeper: if not product-related, we're done.\n",
        "    if is_related_label == 'LABEL_0': # LABEL_0 is \"Not Product Related\"\n",
        "        return {\n",
        "            \"comment\": comment_text,\n",
        "            \"is_related\": \"No\",\n",
        "            \"category\": \"N/A\",\n",
        "            \"brands\": \"N/A\",\n",
        "            \"relevance_score\": stage1_result['score']\n",
        "        }\n",
        "\n",
        "    # If it IS product-related, proceed to Stage 2\n",
        "    # Stage 2: What category is it?\n",
        "    stage2_result = classifier_stage2(comment_text)[0]\n",
        "    # The pipeline returns a string like 'LABEL_2'. We need to extract the number.\n",
        "    label_id_stage2 = int(stage2_result['label'].split('_')[1])\n",
        "    category = stage2_id2label[label_id_stage2]\n",
        "\n",
        "    # Stage 3: Which brands are mentioned?\n",
        "    brands = extract_brands(comment_text, brand_keywords)\n",
        "\n",
        "    return {\n",
        "        \"comment\": comment_text,\n",
        "        \"is_related\": \"Yes\",\n",
        "        \"category\": category,\n",
        "        \"brands\": \", \".join(brands), # Join list into a clean string\n",
        "        \"relevance_score\": stage1_result['score'],\n",
        "        \"category_score\": stage2_result['score']\n",
        "    }"
      ],
      "metadata": {
        "id": "fR1NTAt6KgNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Sample Comments\n",
        "comments = [\n",
        "    \"This foundation blends so smoothly, perfect for my skin tone.\",\n",
        "    \"The lipstick color is beautiful but it fades too quickly.\",\n",
        "    \"I love how this moisturizer keeps my skin hydrated all day.\",\n",
        "    \"Is this serum safe for sensitive skin?\",\n",
        "    \"The packaging is cute but the mascara dries out too fast.\",\n",
        "    \"This sunscreen leaves a white cast, not good for darker skin tones.\",\n",
        "    \"Best L'OrÃ©al Elvive shampoo Iâ€™ve tried, makes my hair feel so soft.\", # Added brands\n",
        "    \"I bought this CeraVe face wash and it really reduced my acne.\", # Added brand\n",
        "    \"The fragrance of this body lotion is too strong for me.\",\n",
        "    \"Does this eye cream actually reduce dark circles?\",\n",
        "    \"Great video, thanks for the explanation!\",\n",
        "    \"The background music is really calming.\",\n",
        "    \"I laughed so hard at the blooper at the end.\",\n",
        "    \"Can you make a tutorial about studying tips?\",\n",
        "    \"Subscribed, looking forward to your next video!\",\n",
        "    \"I tried this new thing and it was okayâ€¦\" ,\n",
        "    \"The packaging looks nice but I havenâ€™t used it yet.\",\n",
        "    \"Not sure if this really works as advertised.\",\n",
        "    \"My skin felt weird after using itâ€¦\",\n",
        "    \"This color is amazing!\" ,\n",
        "    \"I usually donâ€™t like these things, but this is different.\" ,\n",
        "    \"Feels smooth, but not sure if Iâ€™ll buy again.\",\n",
        "    \"It smells strong, not really my style.\" ,\n",
        "    \"The effect is subtle, but noticeable.\" ,\n",
        "    \"Everyone says this is good, but I donâ€™t see the difference.\"\n",
        "]\n",
        "\n",
        "print(\"\\n--- Analyzing Comments with the Full Pipeline ---\")\n",
        "full_results = [analyze_comment(c) for c in comments]\n",
        "\n",
        "# This creates a much more informative and clean final output\n",
        "df_pipeline_results = pd.DataFrame(full_results)\n",
        "\n",
        "print(df_pipeline_results)\n",
        "\n",
        "# Save your final results to a new CSV\n",
        "output_path = \"/content/drive/MyDrive/datathon/results/stage1_stage2_distilbert_results.csv\"\n",
        "df_pipeline_results.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"\\nResults successfully saved to {output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y89-YV4KnAY",
        "outputId": "2748347f-18cc-4f78-9e9e-dea379fcea93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Analyzing Comments with the Full Pipeline ---\n",
            "                                              comment is_related   category  \\\n",
            "0   This foundation blends so smoothly, perfect fo...        Yes     makeup   \n",
            "1   The lipstick color is beautiful but it fades t...        Yes     makeup   \n",
            "2   I love how this moisturizer keeps my skin hydr...        Yes   haircare   \n",
            "3              Is this serum safe for sensitive skin?        Yes   haircare   \n",
            "4   The packaging is cute but the mascara dries ou...        Yes     makeup   \n",
            "5   This sunscreen leaves a white cast, not good f...        Yes   haircare   \n",
            "6   Best L'OrÃ©al Elvive shampoo Iâ€™ve tried, makes ...        Yes   skincare   \n",
            "7   I bought this CeraVe face wash and it really r...        Yes   haircare   \n",
            "8   The fragrance of this body lotion is too stron...        Yes  haircolor   \n",
            "9   Does this eye cream actually reduce dark circles?        Yes     makeup   \n",
            "10           Great video, thanks for the explanation!         No        N/A   \n",
            "11            The background music is really calming.         No        N/A   \n",
            "12       I laughed so hard at the blooper at the end.         No        N/A   \n",
            "13       Can you make a tutorial about studying tips?         No        N/A   \n",
            "14    Subscribed, looking forward to your next video!         No        N/A   \n",
            "15            I tried this new thing and it was okayâ€¦         No        N/A   \n",
            "16  The packaging looks nice but I havenâ€™t used it...         No        N/A   \n",
            "17       Not sure if this really works as advertised.         No        N/A   \n",
            "18                 My skin felt weird after using itâ€¦         No        N/A   \n",
            "19                             This color is amazing!         No        N/A   \n",
            "20  I usually donâ€™t like these things, but this is...         No        N/A   \n",
            "21      Feels smooth, but not sure if Iâ€™ll buy again.         No        N/A   \n",
            "22             It smells strong, not really my style.         No        N/A   \n",
            "23              The effect is subtle, but noticeable.         No        N/A   \n",
            "24  Everyone says this is good, but I donâ€™t see th...         No        N/A   \n",
            "\n",
            "                 brands  relevance_score  category_score  \n",
            "0   Brand Not Specified         0.999921        0.946392  \n",
            "1   Brand Not Specified         0.999939        0.999476  \n",
            "2   Brand Not Specified         0.971928        0.952503  \n",
            "3   Brand Not Specified         0.999111        0.989260  \n",
            "4   Brand Not Specified         0.999431        0.999145  \n",
            "5   Brand Not Specified         0.999529        0.984866  \n",
            "6       l'orÃ©al, elvive         0.999613        0.985566  \n",
            "7                cerave         0.902343        0.401186  \n",
            "8   Brand Not Specified         0.997084        0.448383  \n",
            "9   Brand Not Specified         0.997522        0.990072  \n",
            "10                  N/A         0.999788             NaN  \n",
            "11                  N/A         0.999791             NaN  \n",
            "12                  N/A         0.999589             NaN  \n",
            "13                  N/A         0.994472             NaN  \n",
            "14                  N/A         0.999567             NaN  \n",
            "15                  N/A         0.999740             NaN  \n",
            "16                  N/A         0.998989             NaN  \n",
            "17                  N/A         0.999615             NaN  \n",
            "18                  N/A         0.999509             NaN  \n",
            "19                  N/A         0.607999             NaN  \n",
            "20                  N/A         0.998156             NaN  \n",
            "21                  N/A         0.999268             NaN  \n",
            "22                  N/A         0.998094             NaN  \n",
            "23                  N/A         0.995324             NaN  \n",
            "24                  N/A         0.998761             NaN  \n",
            "\n",
            "Results successfully saved to /content/drive/MyDrive/datathon/results/stage1_stage2_distilbert_results.csv\n"
          ]
        }
      ]
    }
  ]
}
