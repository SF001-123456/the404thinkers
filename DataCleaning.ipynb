{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82350914",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae792ff9-b4d6-4739-982d-6cb0c766be22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in ./venv/lib/python3.10/site-packages (2.14.1)\n",
      "Requirement already satisfied: transformers in ./venv/lib/python3.10/site-packages (4.56.0)\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.10/site-packages (2.8.0)\n",
      "Requirement already satisfied: nltk in ./venv/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./venv/lib/python3.10/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./venv/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.10/site-packages (from transformers) (2025.8.29)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./venv/lib/python3.10/site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./venv/lib/python3.10/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./venv/lib/python3.10/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./venv/lib/python3.10/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./venv/lib/python3.10/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./venv/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./venv/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./venv/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.10/site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./venv/lib/python3.10/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./venv/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./venv/lib/python3.10/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: triton==3.4.0 in ./venv/lib/python3.10/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./venv/lib/python3.10/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.10/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./venv/lib/python3.10/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./venv/lib/python3.10/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./venv/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in ./venv/lib/python3.10/site-packages (from triton==3.4.0->torch) (59.6.0)\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.10/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.10/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests->transformers) (3.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install emoji transformers torch nltk contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d949ac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import Libraries ---\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import emoji\n",
    "import glob\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c169df7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK data...\n",
      "NLTK data downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# --- Initial Setup and NLTK Downloads ---\n",
    "# Download necessary NLTK data. You only need to run this once.\n",
    "print(\"Downloading NLTK data...\")\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "print(\"NLTK data downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5421fc",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26d32f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your processed chunks directory\n",
    "PROCESSED_DIR = 'datasets/processed_chunks/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52d0852",
   "metadata": {},
   "source": [
    "### Drop Unwanted Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37eb01e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48 Parquet files to update.\n",
      "Starting to drop specified columns from each file...\n",
      "  > Processing chunk 1/48: processed_chunk_1.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > Processing chunk 2/48: processed_chunk_10.parquet\n",
      "  > Processing chunk 3/48: processed_chunk_11.parquet\n",
      "  > Processing chunk 4/48: processed_chunk_12.parquet\n",
      "  > Processing chunk 5/48: processed_chunk_13.parquet\n",
      "  > Processing chunk 6/48: processed_chunk_14.parquet\n",
      "  > Processing chunk 7/48: processed_chunk_15.parquet\n",
      "  > Processing chunk 8/48: processed_chunk_16.parquet\n",
      "  > Processing chunk 9/48: processed_chunk_17.parquet\n",
      "  > Processing chunk 10/48: processed_chunk_18.parquet\n",
      "  > Processing chunk 11/48: processed_chunk_19.parquet\n",
      "  > Processing chunk 12/48: processed_chunk_2.parquet\n",
      "  > Processing chunk 13/48: processed_chunk_20.parquet\n",
      "  > Processing chunk 14/48: processed_chunk_21.parquet\n",
      "  > Processing chunk 15/48: processed_chunk_22.parquet\n",
      "  > Processing chunk 16/48: processed_chunk_23.parquet\n",
      "  > Processing chunk 17/48: processed_chunk_24.parquet\n",
      "  > Processing chunk 18/48: processed_chunk_25.parquet\n",
      "  > Processing chunk 19/48: processed_chunk_26.parquet\n",
      "  > Processing chunk 20/48: processed_chunk_27.parquet\n",
      "  > Processing chunk 21/48: processed_chunk_28.parquet\n",
      "  > Processing chunk 22/48: processed_chunk_29.parquet\n",
      "  > Processing chunk 23/48: processed_chunk_3.parquet\n",
      "  > Processing chunk 24/48: processed_chunk_30.parquet\n",
      "  > Processing chunk 25/48: processed_chunk_31.parquet\n",
      "  > Processing chunk 26/48: processed_chunk_32.parquet\n",
      "  > Processing chunk 27/48: processed_chunk_33.parquet\n",
      "  > Processing chunk 28/48: processed_chunk_34.parquet\n",
      "  > Processing chunk 29/48: processed_chunk_35.parquet\n",
      "  > Processing chunk 30/48: processed_chunk_36.parquet\n",
      "  > Processing chunk 31/48: processed_chunk_37.parquet\n",
      "  > Processing chunk 32/48: processed_chunk_38.parquet\n",
      "  > Processing chunk 33/48: processed_chunk_39.parquet\n",
      "  > Processing chunk 34/48: processed_chunk_4.parquet\n",
      "  > Processing chunk 35/48: processed_chunk_40.parquet\n",
      "  > Processing chunk 36/48: processed_chunk_41.parquet\n",
      "  > Processing chunk 37/48: processed_chunk_42.parquet\n",
      "  > Processing chunk 38/48: processed_chunk_43.parquet\n",
      "  > Processing chunk 39/48: processed_chunk_44.parquet\n",
      "  > Processing chunk 40/48: processed_chunk_45.parquet\n",
      "  > Processing chunk 41/48: processed_chunk_46.parquet\n",
      "  > Processing chunk 42/48: processed_chunk_47.parquet\n",
      "  > Processing chunk 43/48: processed_chunk_48.parquet\n",
      "  > Processing chunk 44/48: processed_chunk_5.parquet\n",
      "  > Processing chunk 45/48: processed_chunk_6.parquet\n",
      "  > Processing chunk 46/48: processed_chunk_7.parquet\n",
      "  > Processing chunk 47/48: processed_chunk_8.parquet\n",
      "  > Processing chunk 48/48: processed_chunk_9.parquet\n",
      "\n",
      "✅ SUCCESS! All 48 chunk files have been updated.\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all the chunked Parquet files\n",
    "parquet_files = sorted(glob.glob(os.path.join(PROCESSED_DIR, 'processed_chunk_*.parquet')))\n",
    "\n",
    "# Define the list of columns you want to drop\n",
    "columns_to_drop = [\n",
    "    'kind_comment', 'commentId', 'channelId_comment', 'videoId', 'authorId',\n",
    "    'parentCommentId', 'kind_video', 'channelId_video',\n",
    "    'title', 'description', 'tags', 'defaultLanguage', 'defaultAudioLanguage', \n",
    "    'contentDuration', 'topicCategories', 'cleaned_text'\n",
    "]\n",
    "\n",
    "# --- PART 1: Drop Columns from Each Parquet File ---\n",
    "print(f\"Found {len(parquet_files)} Parquet files to update.\")\n",
    "print(\"Starting to drop specified columns from each file...\")\n",
    "\n",
    "for i, file_path in enumerate(parquet_files):\n",
    "    print(f\"  > Processing chunk {i+1}/{len(parquet_files)}: {os.path.basename(file_path)}\")\n",
    "    \n",
    "    # Load one chunk\n",
    "    df_chunk = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Drop the columns, using errors='ignore' in case a column is already missing\n",
    "    df_chunk = df_chunk.drop(columns=columns_to_drop, errors='ignore')\n",
    "    \n",
    "    # Overwrite the original file with the updated DataFrame\n",
    "    df_chunk.to_parquet(file_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ SUCCESS! All {len(parquet_files)} chunk files have been updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c48cdb",
   "metadata": {},
   "source": [
    "### Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "689fe5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying columns and checking for missing values across all chunks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns in the updated Parquet files:\n",
      "['textOriginal', 'likeCount_comment', 'publishedAt_comment', 'updatedAt', 'quarter', 'publishedAt_video', 'viewCount', 'likeCount_video', 'favouriteCount', 'commentCount', 'comment_length', 'is_reply', 'video_topics', 'duration_seconds', 'textAvailable']\n",
      "\n",
      "Missing value count for every column across all files:\n",
      "textOriginal                0\n",
      "likeCount_comment           0\n",
      "publishedAt_comment         0\n",
      "updatedAt                   0\n",
      "quarter                     0\n",
      "publishedAt_video        1529\n",
      "viewCount               47557\n",
      "likeCount_video        253707\n",
      "favouriteCount          47557\n",
      "commentCount            47557\n",
      "comment_length              0\n",
      "is_reply                    0\n",
      "video_topics                0\n",
      "duration_seconds        47557\n",
      "textAvailable               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- PART 2: Verify Columns and Check for Missing Values ---\n",
    "print(\"\\nVerifying columns and checking for missing values across all chunks...\")\n",
    "\n",
    "# Check the schema of the first file to confirm columns were dropped\n",
    "if parquet_files:\n",
    "    first_chunk_df = pd.read_parquet(parquet_files[0])\n",
    "    print(\"\\nColumns in the updated Parquet files:\")\n",
    "    print(first_chunk_df.columns.to_list())\n",
    "\n",
    "# Initialize a dictionary to hold the total count of missing values for each column\n",
    "total_missing_values = {}\n",
    "\n",
    "# Loop through the updated files again to count NaNs\n",
    "for i, file_path in enumerate(parquet_files):\n",
    "    # Load the chunk\n",
    "    df_chunk = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Calculate missing values for the current chunk\n",
    "    missing_in_chunk = df_chunk.isnull().sum()\n",
    "    \n",
    "    # Add the chunk's missing counts to the total\n",
    "    for col, count in missing_in_chunk.items():\n",
    "        if col not in total_missing_values:\n",
    "            total_missing_values[col] = 0\n",
    "        total_missing_values[col] += count\n",
    "\n",
    "# Convert the dictionary to a pandas Series for nice printing\n",
    "missing_values_series = pd.Series(total_missing_values)\n",
    "\n",
    "print(\"\\nMissing value count for every column across all files:\")\n",
    "print(missing_values_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "855b7831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48 Parquet files to clean.\n",
      "Starting to remove rows with missing values from each file...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > Processing chunk 1/48: Removed 5261 rows.\n",
      "  > Processing chunk 2/48: Removed 5419 rows.\n",
      "  > Processing chunk 3/48: Removed 5354 rows.\n",
      "  > Processing chunk 4/48: Removed 5567 rows.\n",
      "  > Processing chunk 5/48: Removed 5360 rows.\n",
      "  > Processing chunk 6/48: Removed 5389 rows.\n",
      "  > Processing chunk 7/48: Removed 5279 rows.\n",
      "  > Processing chunk 8/48: Removed 5284 rows.\n",
      "  > Processing chunk 9/48: Removed 5345 rows.\n",
      "  > Processing chunk 10/48: Removed 5431 rows.\n",
      "  > Processing chunk 11/48: Removed 5394 rows.\n",
      "  > Processing chunk 12/48: Removed 5383 rows.\n",
      "  > Processing chunk 13/48: Removed 5368 rows.\n",
      "  > Processing chunk 14/48: Removed 5380 rows.\n",
      "  > Processing chunk 15/48: Removed 5461 rows.\n",
      "  > Processing chunk 16/48: Removed 5396 rows.\n",
      "  > Processing chunk 17/48: Removed 5506 rows.\n",
      "  > Processing chunk 18/48: Removed 5309 rows.\n",
      "  > Processing chunk 19/48: Removed 5339 rows.\n",
      "  > Processing chunk 20/48: Removed 5316 rows.\n",
      "  > Processing chunk 21/48: Removed 5318 rows.\n",
      "  > Processing chunk 22/48: Removed 5286 rows.\n",
      "  > Processing chunk 23/48: Removed 5363 rows.\n",
      "  > Processing chunk 24/48: Removed 5478 rows.\n",
      "  > Processing chunk 25/48: Removed 5443 rows.\n",
      "  > Processing chunk 26/48: Removed 5392 rows.\n",
      "  > Processing chunk 27/48: Removed 5471 rows.\n",
      "  > Processing chunk 28/48: Removed 5284 rows.\n",
      "  > Processing chunk 29/48: Removed 5393 rows.\n",
      "  > Processing chunk 30/48: Removed 5418 rows.\n",
      "  > Processing chunk 31/48: Removed 5382 rows.\n",
      "  > Processing chunk 32/48: Removed 5293 rows.\n",
      "  > Processing chunk 33/48: Removed 5318 rows.\n",
      "  > Processing chunk 34/48: Removed 5304 rows.\n",
      "  > Processing chunk 35/48: Removed 5422 rows.\n",
      "  > Processing chunk 36/48: Removed 5310 rows.\n",
      "  > Processing chunk 37/48: Removed 5347 rows.\n",
      "  > Processing chunk 38/48: Removed 5307 rows.\n",
      "  > Processing chunk 39/48: Removed 5404 rows.\n",
      "  > Processing chunk 40/48: Removed 5448 rows.\n",
      "  > Processing chunk 41/48: Removed 5338 rows.\n",
      "  > Processing chunk 42/48: Removed 5319 rows.\n",
      "  > Processing chunk 43/48: Removed 1341 rows.\n",
      "  > Processing chunk 44/48: Removed 5415 rows.\n",
      "  > Processing chunk 45/48: Removed 5302 rows.\n",
      "  > Processing chunk 46/48: Removed 5391 rows.\n",
      "  > Processing chunk 47/48: Removed 5305 rows.\n",
      "  > Processing chunk 48/48: Removed 5374 rows.\n",
      "\n",
      "✅ SUCCESS! All chunk files have been cleaned.\n",
      "A total of 253707 rows with missing values were removed across all files.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(parquet_files)} Parquet files to clean.\")\n",
    "print(\"Starting to remove rows with missing values from each file...\")\n",
    "\n",
    "# --- PART 1: Remove Rows with Missing Values ---\n",
    "total_rows_removed = 0\n",
    "\n",
    "for i, file_path in enumerate(parquet_files):\n",
    "    # Load one chunk\n",
    "    df_chunk = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Get the number of rows before cleaning\n",
    "    rows_before = len(df_chunk)\n",
    "    \n",
    "    # Drop any row that has at least one missing value (NaN)\n",
    "    df_chunk = df_chunk.dropna()\n",
    "    \n",
    "    # Get the number of rows after cleaning\n",
    "    rows_after = len(df_chunk)\n",
    "    \n",
    "    # Calculate how many rows were removed in this chunk\n",
    "    rows_removed = rows_before - rows_after\n",
    "    total_rows_removed += rows_removed\n",
    "    \n",
    "    print(f\"  > Processing chunk {i+1}/{len(parquet_files)}: Removed {rows_removed} rows.\")\n",
    "    \n",
    "    # Overwrite the original file with the cleaned DataFrame\n",
    "    df_chunk.to_parquet(file_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ SUCCESS! All chunk files have been cleaned.\")\n",
    "print(f\"A total of {total_rows_removed} rows with missing values were removed across all files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01772871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying that no missing values remain...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final verification complete.\n",
      "Total missing values across all files: 0\n",
      "✅ The entire dataset is now clean and contains no missing values.\n"
     ]
    }
   ],
   "source": [
    "# --- PART 2: Final Verification ---\n",
    "print(\"\\nVerifying that no missing values remain...\")\n",
    "\n",
    "# Initialize a counter for any remaining missing values\n",
    "total_missing_count = 0\n",
    "\n",
    "# Loop through the cleaned files to double-check\n",
    "for i, file_path in enumerate(parquet_files):\n",
    "    # Load the cleaned chunk\n",
    "    df_chunk = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Sum up all missing values in this chunk. The result should be 0.\n",
    "    missing_in_chunk = df_chunk.isnull().sum().sum()\n",
    "    total_missing_count += missing_in_chunk\n",
    "\n",
    "print(f\"\\nFinal verification complete.\")\n",
    "print(f\"Total missing values across all files: {total_missing_count}\")\n",
    "\n",
    "if total_missing_count == 0:\n",
    "    print(\"✅ The entire dataset is now clean and contains no missing values.\")\n",
    "else:\n",
    "    print(\"⚠️ Warning: Missing values were still found. Please review the process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca24e16e",
   "metadata": {},
   "source": [
    "### Spam Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c7a6467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Applying Rule-Based Quality Filtering to All Chunks ---\n",
      "  > Processing chunk 1/48: Removed 25506 rows with fewer than 3 words.\n",
      "  > Processing chunk 2/48: Removed 25230 rows with fewer than 3 words.\n",
      "  > Processing chunk 3/48: Removed 25101 rows with fewer than 3 words.\n",
      "  > Processing chunk 4/48: Removed 25166 rows with fewer than 3 words.\n",
      "  > Processing chunk 5/48: Removed 25461 rows with fewer than 3 words.\n",
      "  > Processing chunk 6/48: Removed 25282 rows with fewer than 3 words.\n",
      "  > Processing chunk 7/48: Removed 25417 rows with fewer than 3 words.\n",
      "  > Processing chunk 8/48: Removed 25285 rows with fewer than 3 words.\n",
      "  > Processing chunk 9/48: Removed 25386 rows with fewer than 3 words.\n",
      "  > Processing chunk 10/48: Removed 24945 rows with fewer than 3 words.\n",
      "  > Processing chunk 11/48: Removed 25422 rows with fewer than 3 words.\n",
      "  > Processing chunk 12/48: Removed 25098 rows with fewer than 3 words.\n",
      "  > Processing chunk 13/48: Removed 25336 rows with fewer than 3 words.\n",
      "  > Processing chunk 14/48: Removed 25165 rows with fewer than 3 words.\n",
      "  > Processing chunk 15/48: Removed 25381 rows with fewer than 3 words.\n",
      "  > Processing chunk 16/48: Removed 25365 rows with fewer than 3 words.\n",
      "  > Processing chunk 17/48: Removed 25143 rows with fewer than 3 words.\n",
      "  > Processing chunk 18/48: Removed 25411 rows with fewer than 3 words.\n",
      "  > Processing chunk 19/48: Removed 25376 rows with fewer than 3 words.\n",
      "  > Processing chunk 20/48: Removed 25175 rows with fewer than 3 words.\n",
      "  > Processing chunk 21/48: Removed 25398 rows with fewer than 3 words.\n",
      "  > Processing chunk 22/48: Removed 25309 rows with fewer than 3 words.\n",
      "  > Processing chunk 23/48: Removed 25219 rows with fewer than 3 words.\n",
      "  > Processing chunk 24/48: Removed 25172 rows with fewer than 3 words.\n",
      "  > Processing chunk 25/48: Removed 25358 rows with fewer than 3 words.\n",
      "  > Processing chunk 26/48: Removed 25035 rows with fewer than 3 words.\n",
      "  > Processing chunk 27/48: Removed 25350 rows with fewer than 3 words.\n",
      "  > Processing chunk 28/48: Removed 25370 rows with fewer than 3 words.\n",
      "  > Processing chunk 29/48: Removed 25218 rows with fewer than 3 words.\n",
      "  > Processing chunk 30/48: Removed 25296 rows with fewer than 3 words.\n",
      "  > Processing chunk 31/48: Removed 25331 rows with fewer than 3 words.\n",
      "  > Processing chunk 32/48: Removed 25255 rows with fewer than 3 words.\n",
      "  > Processing chunk 33/48: Removed 25500 rows with fewer than 3 words.\n",
      "  > Processing chunk 34/48: Removed 25304 rows with fewer than 3 words.\n",
      "  > Processing chunk 35/48: Removed 25323 rows with fewer than 3 words.\n",
      "  > Processing chunk 36/48: Removed 25547 rows with fewer than 3 words.\n",
      "  > Processing chunk 37/48: Removed 25221 rows with fewer than 3 words.\n",
      "  > Processing chunk 38/48: Removed 25076 rows with fewer than 3 words.\n",
      "  > Processing chunk 39/48: Removed 25422 rows with fewer than 3 words.\n",
      "  > Processing chunk 40/48: Removed 25169 rows with fewer than 3 words.\n",
      "  > Processing chunk 41/48: Removed 25289 rows with fewer than 3 words.\n",
      "  > Processing chunk 42/48: Removed 25240 rows with fewer than 3 words.\n",
      "  > Processing chunk 43/48: Removed 6292 rows with fewer than 3 words.\n",
      "  > Processing chunk 44/48: Removed 25165 rows with fewer than 3 words.\n",
      "  > Processing chunk 45/48: Removed 25454 rows with fewer than 3 words.\n",
      "  > Processing chunk 46/48: Removed 25402 rows with fewer than 3 words.\n",
      "  > Processing chunk 47/48: Removed 25392 rows with fewer than 3 words.\n",
      "  > Processing chunk 48/48: Removed 25285 rows with fewer than 3 words.\n",
      "\n",
      "✅ SUCCESS! Filtering is complete.\n",
      "A total of 1195043 rows were removed across all files.\n"
     ]
    }
   ],
   "source": [
    "# --- Rule-Based Quality Filtering ---\n",
    "print(\"\\n--- Applying Rule-Based Quality Filtering to All Chunks ---\")\n",
    "\n",
    "# Define the minimum number of words a comment must have to be kept\n",
    "MIN_WORD_COUNT = 3\n",
    "total_rows_removed = 0\n",
    "\n",
    "for i, file_path in enumerate(parquet_files):\n",
    "    # Load one chunk\n",
    "    df_chunk = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Get the number of rows before filtering\n",
    "    rows_before = len(df_chunk)\n",
    "    \n",
    "    # Calculate word count on the 'textOriginal' column\n",
    "    if 'textOriginal' in df_chunk.columns:\n",
    "        word_counts = df_chunk['textOriginal'].str.split().str.len()\n",
    "        \n",
    "        # Apply the filter\n",
    "        df_chunk = df_chunk[word_counts >= MIN_WORD_COUNT].copy()\n",
    "        \n",
    "        # Get the number of rows after filtering\n",
    "        rows_after = len(df_chunk)\n",
    "        \n",
    "        # Calculate how many rows were removed in this chunk\n",
    "        rows_removed = rows_before - rows_after\n",
    "        total_rows_removed += rows_removed\n",
    "        \n",
    "        print(f\"  > Processing chunk {i+1}/{len(parquet_files)}: Removed {rows_removed} rows with fewer than {MIN_WORD_COUNT} words.\")\n",
    "        \n",
    "        # Overwrite the original file with the filtered DataFrame\n",
    "        df_chunk.to_parquet(file_path, index=False)\n",
    "    else:\n",
    "        print(f\"  > Skipping chunk {i+1}/{len(parquet_files)}: 'textOriginal' column not found.\")\n",
    "\n",
    "print(f\"\\n✅ SUCCESS! Filtering is complete.\")\n",
    "print(f\"A total of {total_rows_removed} rows were removed across all files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc196b6b",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac9f1ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define the Preprocessing Function and Initialize Tools ---\n",
    "\n",
    "# Initialize tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Applies a series of preprocessing steps to clean a given text string.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return \"\"   # return empty string if not a valid string\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "\n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r\"[@#]\\w+\", \"\", text)\n",
    "\n",
    "    # Remove timestamps like 1:23 or 00:45\n",
    "    text = re.sub(r\"\\b\\d{1,2}:\\d{2}\\b\", \"\", text)\n",
    "\n",
    "    # Remove emojis\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "    \n",
    "    # Expand contractions (e.g., \"don't\" -> \"do not\")\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Normalize elongated words (\"soooo\" -> \"soo\")\n",
    "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and lemmatize\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Join tokens back into a string\n",
    "    return \" \".join(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bf7c842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 48 Parquet files to preprocess.\n",
      "Starting advanced text preprocessing on the 'textAvailable' column...\n",
      "  > Processing chunk 1/48: processed_chunk_1.parquet\n",
      "  > Processing chunk 2/48: processed_chunk_10.parquet\n",
      "  > Processing chunk 3/48: processed_chunk_11.parquet\n",
      "  > Processing chunk 4/48: processed_chunk_12.parquet\n",
      "  > Processing chunk 5/48: processed_chunk_13.parquet\n",
      "  > Processing chunk 6/48: processed_chunk_14.parquet\n",
      "  > Processing chunk 7/48: processed_chunk_15.parquet\n",
      "  > Processing chunk 8/48: processed_chunk_16.parquet\n",
      "  > Processing chunk 9/48: processed_chunk_17.parquet\n",
      "  > Processing chunk 10/48: processed_chunk_18.parquet\n",
      "  > Processing chunk 11/48: processed_chunk_19.parquet\n",
      "  > Processing chunk 12/48: processed_chunk_2.parquet\n",
      "  > Processing chunk 13/48: processed_chunk_20.parquet\n",
      "  > Processing chunk 14/48: processed_chunk_21.parquet\n",
      "  > Processing chunk 15/48: processed_chunk_22.parquet\n",
      "  > Processing chunk 16/48: processed_chunk_23.parquet\n",
      "  > Processing chunk 17/48: processed_chunk_24.parquet\n",
      "  > Processing chunk 18/48: processed_chunk_25.parquet\n",
      "  > Processing chunk 19/48: processed_chunk_26.parquet\n",
      "  > Processing chunk 20/48: processed_chunk_27.parquet\n",
      "  > Processing chunk 21/48: processed_chunk_28.parquet\n",
      "  > Processing chunk 22/48: processed_chunk_29.parquet\n",
      "  > Processing chunk 23/48: processed_chunk_3.parquet\n",
      "  > Processing chunk 24/48: processed_chunk_30.parquet\n",
      "  > Processing chunk 25/48: processed_chunk_31.parquet\n",
      "  > Processing chunk 26/48: processed_chunk_32.parquet\n",
      "  > Processing chunk 27/48: processed_chunk_33.parquet\n",
      "  > Processing chunk 28/48: processed_chunk_34.parquet\n",
      "  > Processing chunk 29/48: processed_chunk_35.parquet\n",
      "  > Processing chunk 30/48: processed_chunk_36.parquet\n",
      "  > Processing chunk 31/48: processed_chunk_37.parquet\n",
      "  > Processing chunk 32/48: processed_chunk_38.parquet\n",
      "  > Processing chunk 33/48: processed_chunk_39.parquet\n",
      "  > Processing chunk 34/48: processed_chunk_4.parquet\n",
      "  > Processing chunk 35/48: processed_chunk_40.parquet\n",
      "  > Processing chunk 36/48: processed_chunk_41.parquet\n",
      "  > Processing chunk 37/48: processed_chunk_42.parquet\n",
      "  > Processing chunk 38/48: processed_chunk_43.parquet\n",
      "  > Processing chunk 39/48: processed_chunk_44.parquet\n",
      "  > Processing chunk 40/48: processed_chunk_45.parquet\n",
      "  > Processing chunk 41/48: processed_chunk_46.parquet\n",
      "  > Processing chunk 42/48: processed_chunk_47.parquet\n",
      "  > Processing chunk 43/48: processed_chunk_48.parquet\n",
      "  > Processing chunk 44/48: processed_chunk_5.parquet\n",
      "  > Processing chunk 45/48: processed_chunk_6.parquet\n",
      "  > Processing chunk 46/48: processed_chunk_7.parquet\n",
      "  > Processing chunk 47/48: processed_chunk_8.parquet\n",
      "  > Processing chunk 48/48: processed_chunk_9.parquet\n",
      "\n",
      "✅ SUCCESS! Text preprocessing is complete for all 48 files.\n"
     ]
    }
   ],
   "source": [
    "# --- Apply Cleaning Function to All Parquet Chunks ---\n",
    "\n",
    "print(f\"\\nFound {len(parquet_files)} Parquet files to preprocess.\")\n",
    "print(\"Starting advanced text preprocessing on the 'textAvailable' column...\")\n",
    "\n",
    "for i, file_path in enumerate(parquet_files):\n",
    "    print(f\"  > Processing chunk {i+1}/{len(parquet_files)}: {os.path.basename(file_path)}\")\n",
    "    \n",
    "    # Load one chunk\n",
    "    df_chunk = pd.read_parquet(file_path)\n",
    "\n",
    "    # Apply the cleaning function and store the result in a new 'cleaned_text' column\n",
    "    # We use 'textAvailable' as the source, per your instructions.\n",
    "    if 'textAvailable' in df_chunk.columns:\n",
    "        df_chunk['cleaned_text'] = df_chunk['textAvailable'].apply(clean_text)\n",
    "        \n",
    "        # Overwrite the original file with the updated DataFrame\n",
    "        df_chunk.to_parquet(file_path, index=False)\n",
    "    else:\n",
    "        print(f\"  > SKIPPING: 'textAvailable' column not found in this chunk.\")\n",
    "\n",
    "print(f\"\\n✅ SUCCESS! Text preprocessing is complete for all {len(parquet_files)} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c1f8933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying the result by inspecting the first chunk...\n",
      "Inspecting file: processed_chunk_1.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Columns in the updated chunk:\n",
      "Index(['textOriginal', 'likeCount_comment', 'publishedAt_comment', 'updatedAt',\n",
      "       'quarter', 'publishedAt_video', 'viewCount', 'likeCount_video',\n",
      "       'favouriteCount', 'commentCount', 'comment_length', 'is_reply',\n",
      "       'video_topics', 'duration_seconds', 'textAvailable', 'cleaned_text'],\n",
      "      dtype='object')\n",
      "\n",
      "✅ Top 5 rows of the updated chunk:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textOriginal</th>\n",
       "      <th>likeCount_comment</th>\n",
       "      <th>publishedAt_comment</th>\n",
       "      <th>updatedAt</th>\n",
       "      <th>quarter</th>\n",
       "      <th>publishedAt_video</th>\n",
       "      <th>viewCount</th>\n",
       "      <th>likeCount_video</th>\n",
       "      <th>favouriteCount</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>comment_length</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>video_topics</th>\n",
       "      <th>duration_seconds</th>\n",
       "      <th>textAvailable</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PLEASE LESBIAN FLAG I BEG YOU \\n\\nYou would ro...</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-08-15 21:48:52+00:00</td>\n",
       "      <td>2023-08-15 21:48:52+00:00</td>\n",
       "      <td>2023Q3</td>\n",
       "      <td>2023-08-15 21:22:52+00:00</td>\n",
       "      <td>9856583.0</td>\n",
       "      <td>307922.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5901.0</td>\n",
       "      <td>49</td>\n",
       "      <td>False</td>\n",
       "      <td>Lifestyle (sociology), Physical attractiveness</td>\n",
       "      <td>29.0</td>\n",
       "      <td>PLEASE LESBIAN FLAG I BEG YOU \\n\\nYou would ro...</td>\n",
       "      <td>please lesbian flag beg would rock tried hair ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apply mashed potato juice and mixed it with curd</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-10-02 13:08:22+00:00</td>\n",
       "      <td>2023-10-02 13:08:22+00:00</td>\n",
       "      <td>2023Q4</td>\n",
       "      <td>2023-10-01 06:30:15+00:00</td>\n",
       "      <td>1148157.0</td>\n",
       "      <td>55043.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>48</td>\n",
       "      <td>True</td>\n",
       "      <td>Lifestyle (sociology), Physical attractiveness</td>\n",
       "      <td>60.0</td>\n",
       "      <td>Apply mashed potato juice and mixed it with cu...</td>\n",
       "      <td>apply mashed potato juice mixed curd foundatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69 missed calls from mars👽</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-05-31 12:03:12+00:00</td>\n",
       "      <td>2024-05-31 12:03:12+00:00</td>\n",
       "      <td>2024Q2</td>\n",
       "      <td>2023-03-05 17:36:18+00:00</td>\n",
       "      <td>14590307.0</td>\n",
       "      <td>313755.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4226.0</td>\n",
       "      <td>26</td>\n",
       "      <td>False</td>\n",
       "      <td>Lifestyle (sociology), Physical attractiveness</td>\n",
       "      <td>20.0</td>\n",
       "      <td>69 missed calls from mars👽 How To Make Small E...</td>\n",
       "      <td>missed call mar make small eye look bigger mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you look like raven from phenomena raven no cap</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-02-15 22:28:44+00:00</td>\n",
       "      <td>2020-02-15 22:28:44+00:00</td>\n",
       "      <td>2020Q1</td>\n",
       "      <td>2020-01-23 21:00:00+00:00</td>\n",
       "      <td>12347504.0</td>\n",
       "      <td>504342.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19920.0</td>\n",
       "      <td>47</td>\n",
       "      <td>False</td>\n",
       "      <td>Lifestyle (sociology), Physical attractiveness</td>\n",
       "      <td>1025.0</td>\n",
       "      <td>you look like raven from phenomena raven no ca...</td>\n",
       "      <td>look like raven phenomenon raven cap black gir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sahi disha me ja ja raha india ka Future..</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-09-03 06:51:48+00:00</td>\n",
       "      <td>2021-09-03 06:51:48+00:00</td>\n",
       "      <td>2021Q3</td>\n",
       "      <td>2021-08-03 13:00:38+00:00</td>\n",
       "      <td>11552362.0</td>\n",
       "      <td>205104.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>42</td>\n",
       "      <td>False</td>\n",
       "      <td>Lifestyle (sociology), Physical attractiveness</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Sahi disha me ja ja raha india ka Future.. Kau...</td>\n",
       "      <td>sahi disha ja ja raha india ka future kaun kau...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        textOriginal  likeCount_comment  \\\n",
       "0  PLEASE LESBIAN FLAG I BEG YOU \\n\\nYou would ro...                  0   \n",
       "1   Apply mashed potato juice and mixed it with curd                  0   \n",
       "2                         69 missed calls from mars👽                  0   \n",
       "3    you look like raven from phenomena raven no cap                  0   \n",
       "4         Sahi disha me ja ja raha india ka Future..                  0   \n",
       "\n",
       "        publishedAt_comment                  updatedAt quarter  \\\n",
       "0 2023-08-15 21:48:52+00:00  2023-08-15 21:48:52+00:00  2023Q3   \n",
       "1 2023-10-02 13:08:22+00:00  2023-10-02 13:08:22+00:00  2023Q4   \n",
       "2 2024-05-31 12:03:12+00:00  2024-05-31 12:03:12+00:00  2024Q2   \n",
       "3 2020-02-15 22:28:44+00:00  2020-02-15 22:28:44+00:00  2020Q1   \n",
       "4 2021-09-03 06:51:48+00:00  2021-09-03 06:51:48+00:00  2021Q3   \n",
       "\n",
       "          publishedAt_video   viewCount  likeCount_video  favouriteCount  \\\n",
       "0 2023-08-15 21:22:52+00:00   9856583.0         307922.0             0.0   \n",
       "1 2023-10-01 06:30:15+00:00   1148157.0          55043.0             0.0   \n",
       "2 2023-03-05 17:36:18+00:00  14590307.0         313755.0             0.0   \n",
       "3 2020-01-23 21:00:00+00:00  12347504.0         504342.0             0.0   \n",
       "4 2021-08-03 13:00:38+00:00  11552362.0         205104.0             0.0   \n",
       "\n",
       "   commentCount  comment_length  is_reply  \\\n",
       "0        5901.0              49     False   \n",
       "1         164.0              48      True   \n",
       "2        4226.0              26     False   \n",
       "3       19920.0              47     False   \n",
       "4         616.0              42     False   \n",
       "\n",
       "                                     video_topics  duration_seconds  \\\n",
       "0  Lifestyle (sociology), Physical attractiveness              29.0   \n",
       "1  Lifestyle (sociology), Physical attractiveness              60.0   \n",
       "2  Lifestyle (sociology), Physical attractiveness              20.0   \n",
       "3  Lifestyle (sociology), Physical attractiveness            1025.0   \n",
       "4  Lifestyle (sociology), Physical attractiveness              16.0   \n",
       "\n",
       "                                       textAvailable  \\\n",
       "0  PLEASE LESBIAN FLAG I BEG YOU \\n\\nYou would ro...   \n",
       "1  Apply mashed potato juice and mixed it with cu...   \n",
       "2  69 missed calls from mars👽 How To Make Small E...   \n",
       "3  you look like raven from phenomena raven no ca...   \n",
       "4  Sahi disha me ja ja raha india ka Future.. Kau...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  please lesbian flag beg would rock tried hair ...  \n",
       "1  apply mashed potato juice mixed curd foundatio...  \n",
       "2  missed call mar make small eye look bigger mak...  \n",
       "3  look like raven phenomenon raven cap black gir...  \n",
       "4  sahi disha ja ja raha india ka future kaun kau...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Verification Step ---\n",
    "print(\"\\nVerifying the result by inspecting the first chunk...\")\n",
    "\n",
    "# Choose any chunk file to inspect, for example, the first one\n",
    "file_to_check = os.path.join(PROCESSED_DIR, 'processed_chunk_1.parquet')\n",
    "\n",
    "print(f\"Inspecting file: {os.path.basename(file_to_check)}\")\n",
    "\n",
    "# Load just this single, small chunk\n",
    "df_one_chunk = pd.read_parquet(file_to_check)\n",
    "\n",
    "# 1. Check the columns\n",
    "print(\"\\n✅ Columns in the updated chunk:\")\n",
    "print(df_one_chunk.columns)\n",
    "\n",
    "# 2. Check the top 5 rows\n",
    "print(\"\\n✅ Top 5 rows of the updated chunk:\")\n",
    "df_one_chunk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937db7cb",
   "metadata": {},
   "source": [
    "### Final Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb04e6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory 'datasets/' is ready.\n",
      "\n",
      "Found 48 Parquet files to merge.\n",
      "Starting the merge process...\n",
      "  > Loading chunk 1/48: processed_chunk_1.parquet\n",
      "  > Loading chunk 2/48: processed_chunk_10.parquet\n",
      "  > Loading chunk 3/48: processed_chunk_11.parquet\n",
      "  > Loading chunk 4/48: processed_chunk_12.parquet\n",
      "  > Loading chunk 5/48: processed_chunk_13.parquet\n",
      "  > Loading chunk 6/48: processed_chunk_14.parquet\n",
      "  > Loading chunk 7/48: processed_chunk_15.parquet\n",
      "  > Loading chunk 8/48: processed_chunk_16.parquet\n",
      "  > Loading chunk 9/48: processed_chunk_17.parquet\n",
      "  > Loading chunk 10/48: processed_chunk_18.parquet\n",
      "  > Loading chunk 11/48: processed_chunk_19.parquet\n",
      "  > Loading chunk 12/48: processed_chunk_2.parquet\n",
      "  > Loading chunk 13/48: processed_chunk_20.parquet\n",
      "  > Loading chunk 14/48: processed_chunk_21.parquet\n",
      "  > Loading chunk 15/48: processed_chunk_22.parquet\n",
      "  > Loading chunk 16/48: processed_chunk_23.parquet\n",
      "  > Loading chunk 17/48: processed_chunk_24.parquet\n",
      "  > Loading chunk 18/48: processed_chunk_25.parquet\n",
      "  > Loading chunk 19/48: processed_chunk_26.parquet\n",
      "  > Loading chunk 20/48: processed_chunk_27.parquet\n",
      "  > Loading chunk 21/48: processed_chunk_28.parquet\n",
      "  > Loading chunk 22/48: processed_chunk_29.parquet\n",
      "  > Loading chunk 23/48: processed_chunk_3.parquet\n",
      "  > Loading chunk 24/48: processed_chunk_30.parquet\n",
      "  > Loading chunk 25/48: processed_chunk_31.parquet\n",
      "  > Loading chunk 26/48: processed_chunk_32.parquet\n",
      "  > Loading chunk 27/48: processed_chunk_33.parquet\n",
      "  > Loading chunk 28/48: processed_chunk_34.parquet\n",
      "  > Loading chunk 29/48: processed_chunk_35.parquet\n",
      "  > Loading chunk 30/48: processed_chunk_36.parquet\n",
      "  > Loading chunk 31/48: processed_chunk_37.parquet\n",
      "  > Loading chunk 32/48: processed_chunk_38.parquet\n",
      "  > Loading chunk 33/48: processed_chunk_39.parquet\n",
      "  > Loading chunk 34/48: processed_chunk_4.parquet\n",
      "  > Loading chunk 35/48: processed_chunk_40.parquet\n",
      "  > Loading chunk 36/48: processed_chunk_41.parquet\n",
      "  > Loading chunk 37/48: processed_chunk_42.parquet\n",
      "  > Loading chunk 38/48: processed_chunk_43.parquet\n",
      "  > Loading chunk 39/48: processed_chunk_44.parquet\n",
      "  > Loading chunk 40/48: processed_chunk_45.parquet\n",
      "  > Loading chunk 41/48: processed_chunk_46.parquet\n",
      "  > Loading chunk 42/48: processed_chunk_47.parquet\n",
      "  > Loading chunk 43/48: processed_chunk_48.parquet\n",
      "  > Loading chunk 44/48: processed_chunk_5.parquet\n",
      "  > Loading chunk 45/48: processed_chunk_6.parquet\n",
      "  > Loading chunk 46/48: processed_chunk_7.parquet\n",
      "  > Loading chunk 47/48: processed_chunk_8.parquet\n",
      "  > Loading chunk 48/48: processed_chunk_9.parquet\n",
      "\n",
      "Concatenating all chunks into a single DataFrame...\n",
      "Concatenation complete.\n",
      "The final dataset has 3276262 rows.\n",
      "\n",
      "Saving the final DataFrame to: datasets/cleaned_merged_data.csv\n",
      "\n",
      "✅ SUCCESS! The merged CSV file has been created.\n"
     ]
    }
   ],
   "source": [
    "# --- Define Paths ---\n",
    "\n",
    "# Define the path for the final output\n",
    "OUTPUT_DIR = 'datasets/'\n",
    "OUTPUT_CSV_FILE = os.path.join(OUTPUT_DIR, 'cleaned_merged_data.csv')\n",
    "\n",
    "# --- Create Output Directory ---\n",
    "# Ensure the destination folder exists before we try to save to it\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory '{OUTPUT_DIR}' is ready.\")\n",
    "\n",
    "# --- Load and Merge All Parquet Files ---\n",
    "\n",
    "# Get a list of all the chunked Parquet files\n",
    "parquet_files = sorted(glob.glob(os.path.join(PROCESSED_DIR, 'processed_chunk_*.parquet')))\n",
    "\n",
    "if not parquet_files:\n",
    "    print(\"\\nError: No Parquet files found in the processed directory.\")\n",
    "    print(\"Please ensure the previous preprocessing steps ran successfully.\")\n",
    "else:\n",
    "    print(f\"\\nFound {len(parquet_files)} Parquet files to merge.\")\n",
    "    print(\"Starting the merge process...\")\n",
    "\n",
    "    # Create a list to hold each loaded DataFrame chunk\n",
    "    all_chunks_list = []\n",
    "\n",
    "    # Loop through each file path, load the file, and add it to our list\n",
    "    for i, file_path in enumerate(parquet_files):\n",
    "        print(f\"  > Loading chunk {i+1}/{len(parquet_files)}: {os.path.basename(file_path)}\")\n",
    "        df_chunk = pd.read_parquet(file_path)\n",
    "        all_chunks_list.append(df_chunk)\n",
    "\n",
    "    # Concatenate all the DataFrames in the list into one single DataFrame\n",
    "    print(\"\\nConcatenating all chunks into a single DataFrame...\")\n",
    "    final_df = pd.concat(all_chunks_list, ignore_index=True)\n",
    "    \n",
    "    print(\"Concatenation complete.\")\n",
    "    print(f\"The final dataset has {len(final_df)} rows.\")\n",
    "\n",
    "    # --- Save the Merged DataFrame to a CSV File ---\n",
    "    print(f\"\\nSaving the final DataFrame to: {OUTPUT_CSV_FILE}\")\n",
    "    \n",
    "    # Save to CSV, index=False is important to avoid writing row numbers\n",
    "    final_df.to_csv(OUTPUT_CSV_FILE, index=False)\n",
    "\n",
    "    print(f\"\\n✅ SUCCESS! The merged CSV file has been created.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
