{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0e0b37e-556f-473a-9534-e4af43c91b55",
   "metadata": {},
   "source": [
    "### Import Files and Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbe5cc00-18ad-4bf5-b1a0-a27fa2c43c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./venv/lib/python3.10/site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.10/site-packages (3.10.5)\n",
      "Requirement already satisfied: seaborn in ./venv/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: pyarrow==15.0.0 in ./venv/lib/python3.10/site-packages (15.0.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.10/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./venv/lib/python3.10/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.10/site-packages (from matplotlib) (4.59.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy matplotlib seaborn \"pyarrow==15.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6994130-8255-4c35-8ace-e764ea9196a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfe20b51-dbfe-4475-a3e6-9f9d63dd4b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import files\n",
    "FOLDER_PATH = './datasets'\n",
    "COMMENT_FILE = os.path.join(FOLDER_PATH, 'merged_comments.csv')\n",
    "comments_df = pd.read_csv(COMMENT_FILE)\n",
    "VIDEO_FILE = os.path.join(FOLDER_PATH, 'videos.csv')\n",
    "videos_df = pd.read_csv(VIDEO_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c5f5d0-bd88-48f0-9a1b-7f190847c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "# textOriginal\n",
    "def clean_text(text):\n",
    "    \"\"\"Takes raw comment text and prepares it for NLP.\"\"\"\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)      # Remove URLs\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)    # Remove punctuation, numbers, etc.\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "# topicCategory\n",
    "def parse_topics(topic_string):\n",
    "    \"\"\"Extracts clean topic names from the Wikipedia URL list.\"\"\"\n",
    "    if pd.isna(topic_string): return []\n",
    "    try:\n",
    "        topics = re.findall(r'/wiki/([^,\\'\\]]+)', topic_string)\n",
    "        return [topic.replace('_', ' ') for topic in topics]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# PublishedAt & UpdatedAt\n",
    "def parse_duration(duration_str):\n",
    "    \"\"\"Converts ISO 8601 duration string (e.g., 'PT1M30S') into total seconds (e.g., 90).\"\"\"\n",
    "    if not isinstance(duration_str, str) or 'P' not in duration_str: return None\n",
    "    try:\n",
    "        duration_str = duration_str.replace('P', '').replace('T', '')\n",
    "        hours = re.search(r'(\\d+)H', duration_str)\n",
    "        minutes = re.search(r'(\\d+)M', duration_str)\n",
    "        seconds = re.search(r'(\\d+)S', duration_str)\n",
    "        total_seconds = 0\n",
    "        if hours: total_seconds += int(hours.group(1)) * 3600\n",
    "        if minutes: total_seconds += int(minutes.group(1)) * 60\n",
    "        if seconds: total_seconds += int(seconds.group(1))\n",
    "        return total_seconds\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70d7f7b0-b5b6-41b7-9596-d72e7abb260f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting the main processing loop. This will take several minutes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > Processing chunk 1...\n",
      "  > Processing chunk 2...\n",
      "  > Processing chunk 3...\n",
      "  > Processing chunk 4...\n",
      "  > Processing chunk 5...\n",
      "  > Processing chunk 6...\n",
      "  > Processing chunk 7...\n",
      "  > Processing chunk 8...\n",
      "  > Processing chunk 9...\n",
      "  > Processing chunk 10...\n",
      "  > Processing chunk 11...\n",
      "  > Processing chunk 12...\n",
      "  > Processing chunk 13...\n",
      "  > Processing chunk 14...\n",
      "  > Processing chunk 15...\n",
      "  > Processing chunk 16...\n",
      "  > Processing chunk 17...\n",
      "  > Processing chunk 18...\n",
      "  > Processing chunk 19...\n",
      "  > Processing chunk 20...\n",
      "  > Processing chunk 21...\n",
      "  > Processing chunk 22...\n",
      "  > Processing chunk 23...\n",
      "  > Processing chunk 24...\n",
      "  > Processing chunk 25...\n",
      "  > Processing chunk 26...\n",
      "  > Processing chunk 27...\n",
      "  > Processing chunk 28...\n",
      "  > Processing chunk 29...\n",
      "  > Processing chunk 30...\n",
      "  > Processing chunk 31...\n",
      "  > Processing chunk 32...\n",
      "  > Processing chunk 33...\n",
      "  > Processing chunk 34...\n",
      "  > Processing chunk 35...\n",
      "  > Processing chunk 36...\n",
      "  > Processing chunk 37...\n",
      "  > Processing chunk 38...\n",
      "  > Processing chunk 39...\n",
      "  > Processing chunk 40...\n",
      "  > Processing chunk 41...\n",
      "  > Processing chunk 42...\n",
      "  > Processing chunk 43...\n",
      "  > Processing chunk 44...\n",
      "  > Processing chunk 45...\n",
      "  > Processing chunk 46...\n",
      "  > Processing chunk 47...\n",
      "  > Processing chunk 48...\n",
      "\n",
      "âœ… SUCCESS! Processing is complete.\n",
      "Saved 48 fully processed Parquet files to the folder: './datasets/processed_chunks/'\n"
     ]
    }
   ],
   "source": [
    "# Define processed chunks path\n",
    "PROCESSED_DIR = os.path.join(FOLDER_PATH, 'processed_chunks/')\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "    \n",
    "# Process merged_comments.csv into chunks\n",
    "chunk_size = 100000\n",
    "comment_chunks_iterator = pd.read_csv(COMMENT_FILE, chunksize=chunk_size)\n",
    "chunk_num = 0\n",
    "\n",
    "print(\"\\nStarting the main processing loop. This will take several minutes...\")\n",
    "\n",
    "for chunk in comment_chunks_iterator:\n",
    "    chunk_num += 1\n",
    "    print(f\"  > Processing chunk {chunk_num}...\")\n",
    "\n",
    "    # A. Merge the comment chunk with our videos lookup table\n",
    "    merged_chunk = pd.merge(chunk, videos_df, on='videoId', how='left', suffixes=('_comment', '_video'))\n",
    "\n",
    "    # B. Clean up missing text to prevent errors\n",
    "    text_cols = ['textOriginal', 'title', 'description', 'tags']\n",
    "    for col in text_cols:\n",
    "        if col in merged_chunk.columns:\n",
    "            # MODIFICATION 1: Avoid 'inplace=True' which is being deprecated. This is safer.\n",
    "            merged_chunk[col] = merged_chunk[col].fillna(\"\")\n",
    "\n",
    "    # C. Convert date columns from text to actual datetime objects\n",
    "    merged_chunk['publishedAt_comment'] = pd.to_datetime(merged_chunk['publishedAt_comment'], errors='coerce')\n",
    "    merged_chunk['publishedAt_video'] = pd.to_datetime(merged_chunk['publishedAt_video'], errors='coerce')\n",
    "\n",
    "    # D. Create new features using our helper functions\n",
    "    merged_chunk['comment_length'] = merged_chunk['textOriginal'].str.len()\n",
    "    merged_chunk['is_reply'] = merged_chunk['parentCommentId'].notna()\n",
    "    merged_chunk['cleaned_text'] = merged_chunk['textOriginal'].apply(clean_text)\n",
    "    merged_chunk['video_topics'] = merged_chunk['topicCategories'].apply(parse_topics)\n",
    "    merged_chunk['duration_seconds'] = merged_chunk['contentDuration'].apply(parse_duration)\n",
    "\n",
    "    # MODIFICATION 2 (THE FIX): Convert the 'video_topics' list into a simple string.\n",
    "    # We join the list with commas. If a cell is not a list, it becomes an empty string.\n",
    "    merged_chunk['video_topics'] = merged_chunk['video_topics'].apply(\n",
    "        lambda x: ', '.join(x) if isinstance(x, list) else ''\n",
    "    )\n",
    "\n",
    "    # E. Save the final, processed chunk to a new Parquet file\n",
    "    output_file = os.path.join(PROCESSED_DIR, f'processed_chunk_{chunk_num}.parquet')\n",
    "    merged_chunk.to_parquet(output_file, index=False) # Added index=False as it's good practice\n",
    "\n",
    "print(f\"\\nâœ… SUCCESS! Processing is complete.\")\n",
    "print(f\"Saved {chunk_num} fully processed Parquet files to the folder: '{PROCESSED_DIR}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc982b8-2a9e-4ab6-9c58-2101dc76f8b3",
   "metadata": {},
   "source": [
    "### Append Text Columns into a New Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d8f6073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the Parquet file:\n",
      "['kind_comment', 'commentId', 'channelId_comment', 'videoId', 'authorId', 'textOriginal', 'parentCommentId', 'likeCount_comment', 'publishedAt_comment', 'updatedAt', 'quarter', 'kind_video', 'publishedAt_video', 'channelId_video', 'title', 'description', 'tags', 'defaultLanguage', 'defaultAudioLanguage', 'contentDuration', 'viewCount', 'likeCount_video', 'favouriteCount', 'commentCount', 'topicCategories', 'comment_length', 'is_reply', 'cleaned_text', 'video_topics', 'duration_seconds']\n"
     ]
    }
   ],
   "source": [
    "# Define the path to your processed chunks\n",
    "PROCESSED_DIR = 'datasets/processed_chunks/'\n",
    "\n",
    "# We only need to inspect the first file to see the schema\n",
    "first_chunk_file = os.path.join(PROCESSED_DIR, 'processed_chunk_1.parquet')\n",
    "\n",
    "# Check if the file exists before trying to read it\n",
    "if os.path.exists(first_chunk_file):\n",
    "    # Read the metadata from the Parquet file\n",
    "    parquet_file = pq.ParquetFile(first_chunk_file)\n",
    "    \n",
    "    # Print the column names from the file's schema\n",
    "    print(\"Columns in the Parquet file:\")\n",
    "    print(parquet_file.schema.names)\n",
    "else:\n",
    "    print(f\"Error: The file '{first_chunk_file}' was not found.\")\n",
    "    print(\"Please make sure your first processing script ran successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7210d888-b17d-4ca3-bee2-89c1701b45f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48 Parquet files to update in place.\n",
      "  > Updating chunk 1/48: processed_chunk_1.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > Updating chunk 2/48: processed_chunk_10.parquet\n",
      "  > Updating chunk 3/48: processed_chunk_11.parquet\n",
      "  > Updating chunk 4/48: processed_chunk_12.parquet\n",
      "  > Updating chunk 5/48: processed_chunk_13.parquet\n",
      "  > Updating chunk 6/48: processed_chunk_14.parquet\n",
      "  > Updating chunk 7/48: processed_chunk_15.parquet\n",
      "  > Updating chunk 8/48: processed_chunk_16.parquet\n",
      "  > Updating chunk 9/48: processed_chunk_17.parquet\n",
      "  > Updating chunk 10/48: processed_chunk_18.parquet\n",
      "  > Updating chunk 11/48: processed_chunk_19.parquet\n",
      "  > Updating chunk 12/48: processed_chunk_2.parquet\n",
      "  > Updating chunk 13/48: processed_chunk_20.parquet\n",
      "  > Updating chunk 14/48: processed_chunk_21.parquet\n",
      "  > Updating chunk 15/48: processed_chunk_22.parquet\n",
      "  > Updating chunk 16/48: processed_chunk_23.parquet\n",
      "  > Updating chunk 17/48: processed_chunk_24.parquet\n",
      "  > Updating chunk 18/48: processed_chunk_25.parquet\n",
      "  > Updating chunk 19/48: processed_chunk_26.parquet\n",
      "  > Updating chunk 20/48: processed_chunk_27.parquet\n",
      "  > Updating chunk 21/48: processed_chunk_28.parquet\n",
      "  > Updating chunk 22/48: processed_chunk_29.parquet\n",
      "  > Updating chunk 23/48: processed_chunk_3.parquet\n",
      "  > Updating chunk 24/48: processed_chunk_30.parquet\n",
      "  > Updating chunk 25/48: processed_chunk_31.parquet\n",
      "  > Updating chunk 26/48: processed_chunk_32.parquet\n",
      "  > Updating chunk 27/48: processed_chunk_33.parquet\n",
      "  > Updating chunk 28/48: processed_chunk_34.parquet\n",
      "  > Updating chunk 29/48: processed_chunk_35.parquet\n",
      "  > Updating chunk 30/48: processed_chunk_36.parquet\n",
      "  > Updating chunk 31/48: processed_chunk_37.parquet\n",
      "  > Updating chunk 32/48: processed_chunk_38.parquet\n",
      "  > Updating chunk 33/48: processed_chunk_39.parquet\n",
      "  > Updating chunk 34/48: processed_chunk_4.parquet\n",
      "  > Updating chunk 35/48: processed_chunk_40.parquet\n",
      "  > Updating chunk 36/48: processed_chunk_41.parquet\n",
      "  > Updating chunk 37/48: processed_chunk_42.parquet\n",
      "  > Updating chunk 38/48: processed_chunk_43.parquet\n",
      "  > Updating chunk 39/48: processed_chunk_44.parquet\n",
      "  > Updating chunk 40/48: processed_chunk_45.parquet\n",
      "  > Updating chunk 41/48: processed_chunk_46.parquet\n",
      "  > Updating chunk 42/48: processed_chunk_47.parquet\n",
      "  > Updating chunk 43/48: processed_chunk_48.parquet\n",
      "  > Updating chunk 44/48: processed_chunk_5.parquet\n",
      "  > Updating chunk 45/48: processed_chunk_6.parquet\n",
      "  > Updating chunk 46/48: processed_chunk_7.parquet\n",
      "  > Updating chunk 47/48: processed_chunk_8.parquet\n",
      "  > Updating chunk 48/48: processed_chunk_9.parquet\n",
      "\n",
      "âœ… SUCCESS! All 48 chunk files have been updated with the 'textAvailable' column.\n"
     ]
    }
   ],
   "source": [
    "# Define the directory where your processed chunks are stored\n",
    "PROCESSED_DIR = 'datasets/processed_chunks/'\n",
    "\n",
    "# Get a list of all the chunked Parquet files\n",
    "parquet_files = sorted(glob.glob(os.path.join(PROCESSED_DIR, 'processed_chunk_*.parquet')))\n",
    "\n",
    "print(f\"Found {len(parquet_files)} Parquet files to update in place.\")\n",
    "\n",
    "# Loop through each file chunk\n",
    "for i, file_path in enumerate(parquet_files):\n",
    "    print(f\"  > Updating chunk {i+1}/{len(parquet_files)}: {os.path.basename(file_path)}\")\n",
    "    \n",
    "    # Load one chunk (this is memory-safe)\n",
    "    df_chunk = pd.read_parquet(file_path)\n",
    "\n",
    "    # --- Your concatenation logic (same as before) ---\n",
    "    columns_to_concat = ['textOriginal', 'title', 'description', 'tags', 'video_topics']\n",
    "    \n",
    "    for col in columns_to_concat:\n",
    "        if col in df_chunk.columns:\n",
    "            df_chunk[col] = df_chunk[col].fillna(\"\").astype(str)\n",
    "        else:\n",
    "            df_chunk[col] = \"\"\n",
    "\n",
    "    # Create the new 'textAvailable' column\n",
    "    df_chunk[\"textAvailable\"] = (\n",
    "        df_chunk[\"textOriginal\"] + \" \" +\n",
    "        df_chunk[\"title\"] + \" \" +\n",
    "        df_chunk[\"description\"] + \" \" +\n",
    "        df_chunk[\"tags\"] + \" \" +\n",
    "        df_chunk[\"video_topics\"]\n",
    "    ).str.strip()\n",
    "    \n",
    "    # --- Overwrite the original file with the updated DataFrame ---\n",
    "    # We save back to the *exact same file_path*\n",
    "    df_chunk.to_parquet(file_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ… SUCCESS! All {len(parquet_files)} chunk files have been updated with the 'textAvailable' column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a6820d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting file: processed_chunk_1.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Columns in the updated chunk:\n",
      "Index(['kind_comment', 'commentId', 'channelId_comment', 'videoId', 'authorId',\n",
      "       'textOriginal', 'parentCommentId', 'likeCount_comment',\n",
      "       'publishedAt_comment', 'updatedAt', 'quarter', 'kind_video',\n",
      "       'publishedAt_video', 'channelId_video', 'title', 'description', 'tags',\n",
      "       'defaultLanguage', 'defaultAudioLanguage', 'contentDuration',\n",
      "       'viewCount', 'likeCount_video', 'favouriteCount', 'commentCount',\n",
      "       'topicCategories', 'comment_length', 'is_reply', 'cleaned_text',\n",
      "       'video_topics', 'duration_seconds', 'textAvailable'],\n",
      "      dtype='object')\n",
      "\n",
      "âœ… Top 5 rows of the updated chunk:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kind_comment</th>\n",
       "      <th>commentId</th>\n",
       "      <th>channelId_comment</th>\n",
       "      <th>videoId</th>\n",
       "      <th>authorId</th>\n",
       "      <th>textOriginal</th>\n",
       "      <th>parentCommentId</th>\n",
       "      <th>likeCount_comment</th>\n",
       "      <th>publishedAt_comment</th>\n",
       "      <th>updatedAt</th>\n",
       "      <th>...</th>\n",
       "      <th>likeCount_video</th>\n",
       "      <th>favouriteCount</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>topicCategories</th>\n",
       "      <th>comment_length</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>video_topics</th>\n",
       "      <th>duration_seconds</th>\n",
       "      <th>textAvailable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>youtube#comment</td>\n",
       "      <td>1781382</td>\n",
       "      <td>14492</td>\n",
       "      <td>74288</td>\n",
       "      <td>2032536</td>\n",
       "      <td>PLEASE LESBIAN FLAG I BEG YOU \\n\\nYou would ro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-08-15 21:48:52+00:00</td>\n",
       "      <td>2023-08-15 21:48:52+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>307922.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5901.0</td>\n",
       "      <td>['https://en.wikipedia.org/wiki/Lifestyle_(soc...</td>\n",
       "      <td>49</td>\n",
       "      <td>False</td>\n",
       "      <td>please lesbian flag i beg you you would rock it</td>\n",
       "      <td>Lifestyle (sociology), Physical attractiveness</td>\n",
       "      <td>29.0</td>\n",
       "      <td>PLEASE LESBIAN FLAG I BEG YOU \\n\\nYou would ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>youtube#comment</td>\n",
       "      <td>289571</td>\n",
       "      <td>14727</td>\n",
       "      <td>79618</td>\n",
       "      <td>3043229</td>\n",
       "      <td>Apply mashed potato juice and mixed it with curd</td>\n",
       "      <td>3198066.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-10-02 13:08:22+00:00</td>\n",
       "      <td>2023-10-02 13:08:22+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>55043.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>['https://en.wikipedia.org/wiki/Lifestyle_(soc...</td>\n",
       "      <td>48</td>\n",
       "      <td>True</td>\n",
       "      <td>apply mashed potato juice and mixed it with curd</td>\n",
       "      <td>Lifestyle (sociology), Physical attractiveness</td>\n",
       "      <td>60.0</td>\n",
       "      <td>Apply mashed potato juice and mixed it with cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>youtube#comment</td>\n",
       "      <td>569077</td>\n",
       "      <td>3314</td>\n",
       "      <td>51826</td>\n",
       "      <td>917006</td>\n",
       "      <td>69 missed calls from marsðŸ‘½</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-05-31 12:03:12+00:00</td>\n",
       "      <td>2024-05-31 12:03:12+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>313755.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4226.0</td>\n",
       "      <td>['https://en.wikipedia.org/wiki/Lifestyle_(soc...</td>\n",
       "      <td>26</td>\n",
       "      <td>False</td>\n",
       "      <td>missed calls from mars</td>\n",
       "      <td>Lifestyle (sociology), Physical attractiveness</td>\n",
       "      <td>20.0</td>\n",
       "      <td>69 missed calls from marsðŸ‘½ How To Make Small E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>youtube#comment</td>\n",
       "      <td>2957962</td>\n",
       "      <td>5008</td>\n",
       "      <td>58298</td>\n",
       "      <td>1853470</td>\n",
       "      <td>Baaa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-02-13 15:48:37+00:00</td>\n",
       "      <td>2024-02-13 15:48:37+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>11349.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>['https://en.wikipedia.org/wiki/Lifestyle_(soc...</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>baaa</td>\n",
       "      <td>Lifestyle (sociology), Physical attractiveness</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Baaa 20sec beauty test: BLUSH PLACEMENT for YO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>youtube#comment</td>\n",
       "      <td>673093</td>\n",
       "      <td>21411</td>\n",
       "      <td>1265</td>\n",
       "      <td>2584166</td>\n",
       "      <td>you look like raven from phenomena raven no cap</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-02-15 22:28:44+00:00</td>\n",
       "      <td>2020-02-15 22:28:44+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>504342.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19920.0</td>\n",
       "      <td>['https://en.wikipedia.org/wiki/Lifestyle_(soc...</td>\n",
       "      <td>47</td>\n",
       "      <td>False</td>\n",
       "      <td>you look like raven from phenomena raven no cap</td>\n",
       "      <td>Lifestyle (sociology), Physical attractiveness</td>\n",
       "      <td>1025.0</td>\n",
       "      <td>you look like raven from phenomena raven no ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      kind_comment  commentId  channelId_comment  videoId  authorId  \\\n",
       "0  youtube#comment    1781382              14492    74288   2032536   \n",
       "1  youtube#comment     289571              14727    79618   3043229   \n",
       "2  youtube#comment     569077               3314    51826    917006   \n",
       "3  youtube#comment    2957962               5008    58298   1853470   \n",
       "4  youtube#comment     673093              21411     1265   2584166   \n",
       "\n",
       "                                        textOriginal  parentCommentId  \\\n",
       "0  PLEASE LESBIAN FLAG I BEG YOU \\n\\nYou would ro...              NaN   \n",
       "1   Apply mashed potato juice and mixed it with curd        3198066.0   \n",
       "2                         69 missed calls from marsðŸ‘½              NaN   \n",
       "3                                               Baaa              NaN   \n",
       "4    you look like raven from phenomena raven no cap              NaN   \n",
       "\n",
       "   likeCount_comment       publishedAt_comment                  updatedAt  \\\n",
       "0                  0 2023-08-15 21:48:52+00:00  2023-08-15 21:48:52+00:00   \n",
       "1                  0 2023-10-02 13:08:22+00:00  2023-10-02 13:08:22+00:00   \n",
       "2                  0 2024-05-31 12:03:12+00:00  2024-05-31 12:03:12+00:00   \n",
       "3                  0 2024-02-13 15:48:37+00:00  2024-02-13 15:48:37+00:00   \n",
       "4                  0 2020-02-15 22:28:44+00:00  2020-02-15 22:28:44+00:00   \n",
       "\n",
       "   ... likeCount_video favouriteCount commentCount  \\\n",
       "0  ...        307922.0            0.0       5901.0   \n",
       "1  ...         55043.0            0.0        164.0   \n",
       "2  ...        313755.0            0.0       4226.0   \n",
       "3  ...         11349.0            0.0        286.0   \n",
       "4  ...        504342.0            0.0      19920.0   \n",
       "\n",
       "                                     topicCategories comment_length is_reply  \\\n",
       "0  ['https://en.wikipedia.org/wiki/Lifestyle_(soc...             49    False   \n",
       "1  ['https://en.wikipedia.org/wiki/Lifestyle_(soc...             48     True   \n",
       "2  ['https://en.wikipedia.org/wiki/Lifestyle_(soc...             26    False   \n",
       "3  ['https://en.wikipedia.org/wiki/Lifestyle_(soc...              4    False   \n",
       "4  ['https://en.wikipedia.org/wiki/Lifestyle_(soc...             47    False   \n",
       "\n",
       "                                       cleaned_text  \\\n",
       "0   please lesbian flag i beg you you would rock it   \n",
       "1  apply mashed potato juice and mixed it with curd   \n",
       "2                            missed calls from mars   \n",
       "3                                              baaa   \n",
       "4   you look like raven from phenomena raven no cap   \n",
       "\n",
       "                                     video_topics duration_seconds  \\\n",
       "0  Lifestyle (sociology), Physical attractiveness             29.0   \n",
       "1  Lifestyle (sociology), Physical attractiveness             60.0   \n",
       "2  Lifestyle (sociology), Physical attractiveness             20.0   \n",
       "3  Lifestyle (sociology), Physical attractiveness             25.0   \n",
       "4  Lifestyle (sociology), Physical attractiveness           1025.0   \n",
       "\n",
       "                                       textAvailable  \n",
       "0  PLEASE LESBIAN FLAG I BEG YOU \\n\\nYou would ro...  \n",
       "1  Apply mashed potato juice and mixed it with cu...  \n",
       "2  69 missed calls from marsðŸ‘½ How To Make Small E...  \n",
       "3  Baaa 20sec beauty test: BLUSH PLACEMENT for YO...  \n",
       "4  you look like raven from phenomena raven no ca...  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to your processed chunks directory\n",
    "PROCESSED_DIR = 'datasets/processed_chunks/'\n",
    "\n",
    "# Choose any chunk file to inspect, for example, the first one\n",
    "file_to_check = os.path.join(PROCESSED_DIR, 'processed_chunk_1.parquet')\n",
    "\n",
    "print(f\"Inspecting file: {os.path.basename(file_to_check)}\")\n",
    "\n",
    "# Load just this single, small chunk\n",
    "df_one_chunk = pd.read_parquet(file_to_check)\n",
    "\n",
    "# 1. Check the columns\n",
    "print(\"\\nâœ… Columns in the updated chunk:\")\n",
    "print(df_one_chunk.columns)\n",
    "\n",
    "# 2. Check the top 5 rows\n",
    "print(\"\\nâœ… Top 5 rows of the updated chunk:\")\n",
    "df_one_chunk.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
